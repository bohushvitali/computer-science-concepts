- Knowledgebase
  - Clean Architecture
    - Why is a clean, simple, flexible, evolvable, and agile architecture important?
      - Software architecture is the high level structure of a software system, the discipline of creating such structures, and the documentation of these structures.
      - It is the set of structures needed to reason about the software system, and comprises the software elements, the relations between them, and the properties of both elements and relations.
      - In today’s software development world, requirements change, environments change, team members change, technologies change, and so should the architecture of our systems.
      - The architecture defines the parts of a system that are hard and costly to change.
      - Therefore we are in need of a clean, simple, flexible, evolvable, and agile architecture to be able to keep up with all the changes surrounding us.
    - Clean architecture
      - An architecture that allows to replace details and is easy to verify.
      - Entities
        - Entities encapsulate enterprise-wide business rules. An entity can be an object with methods, or it can be a set of data structures and functions.
      - Use cases
        - Use cases orchestrate the flow of data to and from the entities, and direct those entities to use their enterprise-wide business rules to achieve the goals of the use cases.
      - Interface adapters
        - Adapters that convert data from the format most convenient for the use cases and entities, to the format most convenient for some external agency such as a database or the Web.
      - Frameworks and drivers
        - Glue code to connect UI, databases, devices etc. to the inner circles.
      - Program Flow
        - Starts on the outside and ends on the outside, but can go through several layers (user clicks a button, use case loads some entities from DB, entities decide something that is presented on the UI).
    - Architecture Principles
      - (+) Dependency management
        - This principle states that “High-level modules should not depend on low-level modules.
        - Both should depend on abstractions” and “Abstractions should not depend on details. Details should depend on abstractions”.
        - An inner layer should not know anything about upper/outer layers. As a result dependencies can only point inwards.
        - The concentric circles represent different areas of software. In general, the further in you go, the higher level the software becomes.
        - The outer circles are mechanisms. The inner circles are policies.
        - Source code dependencies can only point inwards. Nothing in an inner circle can know anything at all about something in an outer circle.
        - Use dependency inversion to build up the system (classes in an outer circle implement interfaces of an inner circle or listen to events from inner circles).
      - (+) Independent of frameworks. Independence from Details (UI, DB, transport protocol).
        - An architecture should not depend on frameworks, you should be able to swap a framework with the least effort.
        - This allows you to use such frameworks as tools, rather than having to cram your system into their technical constraints.
        - A software system’s core logic should not be affected by changes in UI, Databases, Frameworks, Libraries, etc.
      - (+) Testable
        - The business rules and use cases can be tested without UI, database, Web server, or any other external element.
      - (+) Entities and Use Cases.
        - Entities and use cases are the core of an application. And these layers should not be affected by changes in “detail layers”.
      - (+) The necessity of using adapters and converters.
        - Adapters and converters are used to convert models when they are propagating between layers to make them convenient to work with on a specific layer and do not spread extra dependencies to other layers.
      - (+) Independent of system boundaries (UI, database, …)
        - The UI, database, or any other external element can easily change without any impact on use cases and business rules.
      - (+) Passing data between boundaries.
        - You should always pay attention to objects being passed between layers. An object being passed should be isolated, simple or even just a plain data type without hidden dependencies.
        - You can encounter problems when you are using ORM (Object Relational Mapping) libraries and passing ORM objects outside the boundaries.
    - Simple architecture
      - An architecture that is easy to understand. Simplicity is, however, subjective.
      - (+) Consistent design decisions
        - One problem has one solution. Similar problems are solved similarly.
      - (+) Number of concepts/technologies
        - Simple solutions make use of only a few different concepts and technologies.
      - (+) Number of interactions
        - The less interactions the simpler the design.
        - A reasonable amount of components with only efferent coupling and most of the others with preferably only afferent coupling.
      - (+) Size
        - Small systems/components are easier to grasp than big ones. Build large systems out of small parts.
      - (+) Modularity
        - Build your system by connecting independent modules with a clearly defined interface (e.g. with adapters).
    - Flexible architecture
      - An architecture that supports change.
      - (+) Separation of concerns
        - Divide your system into distinct features with as little overlap in functionality as possible so that they can be combined freely
      - (+) Software reflects user’s mental model
        - When the structure and interactions inside the software match the user’s mental model, changes in the real world can more easily be applied in software.
      - (+) Abstraction
        - Separating ideas from specific implementations provides the flexibility to change the implementation. But beware of `over abstraction`.
      - (+) Interface slimness
        - Fat interfaces between components lead to strong coupling. Design the interfaces to be as slim as possible. But beware of `ambiguous interfaces`.
      - (+) Prefer composition over inheritance
        - Inheritance increases coupling between parent and child, thereby limiting reuse.
      - (+) Tangle-/cycle-free dependencies
        - The dependency graph of the elements of the architecture has no cycles, thus allowing locally bounded changes.
    - Evolvable architecture
      - An architecture that is easy to adapt step by step to keep up with changes.
      - (+) Matches current needs, not the future
        - The architecture of the current system should match the current needs (functional and non-functional) - not some future ones.
        - This results in simpler, easier to understand solutions. Otherwise, the risk of waste is very high.
      - (+) No dead-ends, architecture can be extended/adapted
        - The current architecture should be extendable and adaptable so that future needs can be addressed.
        - When evaluating different alternatives, choose one that is open for change.
      - (+) Architecture agnostic components
        - When components don’t care about which architecture they run in, the architecture can be changed without having to rewrite the components.
      - (+) Sacrificial architecture
        - When the software has outlived its architecture, throw the architecture away and start over.
        - This mindset can be used to build a first version with a very simple architecture, then start over for the next.
      - (+) Rolling refactoring
        - When a new version of a concept is introduced, then the old one is refactored out step by step.
        - There can be at most two versions of a concept in an application (and it should be temporary).
    - Agile architecture
      - An architecture that supports agile software development by enabling the principles of the Agile Manifesto
      - (+) Allow change quickly
        - The architecture allows quick changes through flexibility and evolvability.
      - (+) Verifiable at any time
        - The architecture can be verified (fulfils all quality aspects) at any time (e.g. every Sprint)
      - (+) Rapid deployment
        - The architecture supports continuous and rapid deployment so that stakeholders can give feedback continuously.
      - (+) Always working
        - The system is always working (probably with limited functionality) so that it is potentially shippable any time/at end of Sprint. Use assumptions, simplifications, simulators, shortcuts, hard-coding to build a walking skeleton.
    - Workflow
      - Use a top-down approach to find the architecture.
      - (+) Context
        - What belongs to your system and what does not? Which external services will you use?
      - (+) Break down into parts
        - Split the whole into parts by applying separation of concerns and the single responsibility principle.
      - (+) Communication
        - Which data flows through which call, message or event from one part to another? What are the properties of the channels (sync/async, reliability, …)
      - (+) Repeat for each part
        - Repeat the above-mentioned three steps for each part as if it were your system.
        - A part is a bounded context, subsystem or component.
    - Defer decisions
      - Decide only things you have enough knowledge about. Otherwise find a way to defer the decision and build up more knowledge. A good architecture allows you to defer most decisions.
      - (+) Abstraction
        - Use an abstraction to hide details so that you don’t have to decide about the details, but can use a simulation/fake at first to build up more knowledge.
      - (+) Simplification
        - Simplify the problem so that a decision can be made and work can progress. Use this to break free from a blocking state, but be aware of the risks a wrong decision could have.
      - (+) Wilful ignorance
        - Refuse to decide and wait until more knowledge about the problem and its potential solutions is built up.
      - (+) Decision delegation
        - Build the (part of a) system in a way that doesn’t require any decision, by making some other (part of the) system responsible that can be implemented later.
        - E.g. instead of deciding how to persist data, make the code calling your code responsible for passing all needed data to your code.
        - This allows you to build your whole business logic and decide about persistence when implementing the host that runs the business logic.
    - Architecture influencing forces
      - (+) Quality attributes
        - The needed quality attributes (functionality, reliability, usability, efficiency, maintainability, portability, …) are the primary drivers for architectural decisions.
      - (+) Team know-how and skills
        - The whole team understands and supports architecture and can make design decisions according to the architecture.
      - (+) Easiness of implementation
        - How easy an envisioned architecture can be implemented is a quality attribute.
      - (+) Cost of operations
        - Most costs of a software system accrue during operations, not implementation.
      - (+) Risks
        - Every technology, library, and design decision has its risks.
      - (+) Inherent opportunities
        - Things the architecture would allow us to do (but without investing any additional effort because we may never need it).
      - (+) Technology churn
        - Availability of new (better) technologies, resulting in a need for architecture change.
      - (+) Trade-offs
        - Designing an architecture comprises making trade-offs between conflicting goals.
        - Trade-offs must reflect the priorities of quality attributes set by the stakeholders.
        - Trade-offs should be documented and communicated to all stakeholders.
    - Architecture degrading forces
      - (-) Architectural drift
        - Introduction of design decisions into a system’s actual architecture that are not included in, encompassed by, or implied by the planned architecture.
      - (-) Architectural erosion
        - Introduction of design decisions into a system’s actual architecture that violate its planned architecture.
    - Architecture killers
      - (-) Split brain
        - Different parts of the system claim ownership of the same data or their interpretation resulting in inconsistencies and difficult synchronisation.
      - (-) Coupling in space and time
        - E.g. shared code to remove duplication hinders independent advancements, a service that needs other services to be up and running.
        - An `initialise` method that has to be called prior to any other method on the class (better use constructor injection or a factory).
      - (-) Dead-end
        - A design decision that prevents further adaptability without a major refactoring or rewrite.
    - Priorities
      - (+) Simplicity before generality
        - Concrete implementations are easier to understand than generalised concepts.
      - (+) Hard-coded before configurable
        - Configurability leads to if/else constructs or polymorphism inside the code, resulting in more complicated code.
      - (+) Use before reuse
        - Don’t design for reuse before the code has never actually been used. This leads to overgeneralisation, inapt interfaces and increased complexity.
      - (+) Working before optimised
        - First, make it work, then optimise. Premature optimisation leads to more complex solutions or to local instead of global optimisations.
      - (+) Quality attributes before functional requirements
        - Use quality scenarios to guide your architectural decisions because most of the times, quality attributes have more impact than functional requirements.
      - (+) Combined small systems over building a single big system
        - Big systems are more complicated to comprehend than a combination of small systems. But beware of complexity hidden in the communication between the systems.
    - Principles
      - (+) The teams that code the system, design the system.
        - Teams themselves are empowered to define, develop, and deliver software, and they are held accountable for the results.
      - (+) Build the simplest architecture that can possibly work.
        - Simplicity leads to comprehensibility, changeability, low defect introduction.
      - (+) When in doubt, code it out.
        - Get real feedback from running code, then decide.
      - (+) They build it, they test it.
        - Testing is an integral part of building software, not an afterthought.
      - (+) System architecture is a role collaboration.
        - The whole team participates in architecture decisions.
      - (+) There is no monopoly on innovation.
        - Every team member has time to innovate (spikes, hackathons, pet project).
    - Tips and tricks
      - (+) Start with concepts, not with technologies.
        - Don’t think in technologies, think in concepts. Then choose technologies matching the concepts and adapt concepts to technological limitations.
      - (+) Think about your envisioned architecture, but also lay a way from here to there.
        - Break your architecture work into steps. Use assumptions and simplifications in early steps.
        - Always make sure that there is a path from the current architecture to the envisioned architecture.
      - (+) Most of the time, persistence is a secondary thought
        - You always have some data. But that is no reason to start your design with the database. Business logic and workflows are more important.
      - (+) Decouple from environment
        - Design everything so that it has to know nothing about its environment.
      - (+) Prototypes, proof of concepts, feasibility studies
        - Break risks and grow knowledge fast, then decide.
      - (+) Use architecture patterns as inspiration, not as solutions.
        - Architecture patterns are good examples of solutions to specific problems.
        - Use them to find solutions for your problems and do not apply them to your problems.
    - Architectural aspects
      - (+) Persistence
        - Form of data (document-based, relational, graph, key-value), backup, transactions, size of data, throughput, replication, availability, concurrency.
      - (+) Translation (UI and data)
        - Static (e.g. resources) vs. dynamic, switchable during implementation/installation/start-up/runtime.
      - (+) Communication between parts
        - Asynchronous/synchronous, un-/reliable, latency, throughput, availability of connection, method calls/events/messages.
      - (+) Scaling
        - Run on multiple threads/processes/machines, availability, consistency, redundancy.
      - (+) Security
        - Authentication, authorisation, threats, encryption (of communication and data).
      - (+) Journaling, auditing
        - Operations, granularity, access to journal, tampering, regulatory.
      - (+) Reporting
        - Access to data (production/dedicated database/data warehouse), delivery mechanism (synchronous/asynchronous), formats (Web, PDF, …).
      - (+) Data migration, data import
        - Available time frame for migration/import, data quality, default values for missing values, value merging/splitting.
      - (+) Releasability
        - Release as one, per service or per component (e.g. plug-in). Automatic or manual release.
      - (+) Versioning
        - One product vs. a product family, technical/marketing version, manually or automatically generated, releases/service packs/hot fixes, SemVer.
      - (+) Backward compatibility
        - APIs, data (input/output/persisted), environment (e.g. old OS).
      - (+) Response times
        - Service time (actually performing the work) + wait time + transmission time.
      - (+) Archiving data
        - Data growth rate, access to archived data, split relations in relational data.
      - (+) Distribution
        - Beware of the fallacies of distributed computing: the network is reliable, latency is zero, bandwidth is infinite, the network is secure, topology doesn’t change, there is one administrator, transport cost is zero, the network is homogeneous.
      - (+) Public interfaces
        - Versioning, immutability and stability of contracts and schemas.
    - Documentation
      - (+) Questions to ask yourself
        - Who is the consumer? What do they need? How do you deliver the documentation to them? How do you know when they are ready for it? How do you produce it? What input do you need to produce it?
      - (+) Manual and automatic production
        - Manual
          - Someone writes the documentation, high risk of being out-of-date, very flexible.
        - Automatic
          - Generated from code, can be regenerated anytime and is therefore never out of date, finding right level of abstraction is hard. 
          - Works good for state machines, bootstrapping mechanics, and structural breakdown.
      - (+) About now, not the future
        - Only document what you did, not what you want to do.
      - (+) Shared
        - The whole team participates in producing the documentation.
    - Architecture smells
      - Causes: applying a design solution in an inappropriate context, mixing design fragments that have undesirable emergent behaviours.
      - (-) Overlayered architecture
        - When there are layers on layers on layers on layers ... in your application.
        - Not providing abstraction, lots of boilerplate code.
      - (-) Overabstraction
        - Too abstract to be understandable. Concrete designs are easier to understand.
      - (-) Overconfigurability
        - Everything is configurable because no decisions were made how the software should behave.
      - (-) Overkill Architecture
        - A simple problem with a complex (however technically interesting) solution.
      - (-) Futuristic Architecture
        - The architecture wants to anticipate a lot of future possible changes. This adds complexity and most likely also waste.
      - (-) Technology enthusiastic Architecture
        - Lots of new cool technology is introduced just for the sake of it.
      - (-) Paper tiger architecture
        - The architecture exists only on paper (UML diagrams) with no connection to the reality.
      - (-) Connector Envy
        -  Components with Connector Envy encompass extensive interaction-related functionality that should be delegated to a connector.
        - A component doing the job that should be delegated to a connector
          - communication (transfer of data)
          - coordination (transfer of control)
          - conversion (bridge different data formats, types, protocols)
          - facilitation (load-balancing, monitoring, fault tolerance)
      - (-) Component Envy
        - Connectors with Component Envy encompass extensive applicationspecific functionality that should be performed by a component.
      - (-) Concern overload
        - Indicates that a component implements an excessive number of concerns.
      - (-) Link Overload 
        - Dependency-based smell that occurs when a component has interfaces involved in an excessive number of links (i.e., dependencies on other components), affecting the system’s separation of concerns and effective isolation of changes.
        - A component may have an excessive number of incoming links, outgoing links, or both.
      - (-) Unused Interface.
        - A brick’s interface is unused if that interface is not used by another brick.
      - (-) Duplicate Component Functionality.
        - A component has duplicated functionality if it shares the same functionality as another component.
      - (-) Scattered parasitic functionality
        - Concern-based architectural smell.
        - It describes a system in which multiple components are responsible for realizing the same high-level concern while some of those components are also responsible for additional, orthogonal concerns.
        - Such an orthogonal concern “infects” a component, akin to a parasite.
        - A single concern is scattered across multiple components and at least one component addresses multiple orthogonal concerns.
      - (-) Ambiguous interfaces
        - Interfaces or meta-interfaces that offer only a single, general entry-point to a component or connector (e.g. pass an object, or general purpose events over an event bus); contain a single parameter; and dispatch to different internal operations based on the content of the parameter.
        - They are not explorable.
      - (-) Extraneous Adjacent Connector
        - Occurs when two connectors of different types are used to link a pair of components.
        - Two connectors of different types are used to link a pair of components.
        - E.g. event (asynchronous) and service call (synchronous).
          - Event: loosely coupled -> availability, replicability.
          - Method call: easy to understand.
          - Both: neither.
      - Dependency cycle indicates a set of components whose links form a circular chain, causing changes to one component to possibly affect all other components in the cycle.
      - Unused Brick. A brick is unused if its interfaces are all unused interfaces.
      - Connector Dimension Overload.
        - Occurs when a connector or set of linked connectors contains an excessive number of connector dimensions or sub-dimensions.
        - For example, a set of connectors that are linked and perform authentication, authorization, encryption, streaming, data access and distribution.
      - Lego Syndrome.
        - A brick suffers from Lego Syndrome when it handles an extremely small amount of functionality or a minor concern that is encompassed by a system concern.
        - This smell type represents bricks that are excessively small.
      - Sloppy Delegation.
        - Occurs when a component delegates to another component a small amount of functionality that it could have performed itself.
        - For example, a component that stores an aircraft’s current velocity, fuel level, and altitude and passes that data to another component that solely calculates that aircraft’s burn rate.
      - Brick Functionality Overload.
        - A brick that performs an excessive amount of functionality suffers from Brick Functionality Overload.
        - Note that the functionality performed by a brick is not necessarily mapped one-to-one to that brick’s concerns.
    - Networking Concepts
    - Networking Models
      - OSI Model
        - The Open Systems Interc­onn­ection model (OSI Model) is a conceptual model that charac­terizes and standa­rdizes the commun­ication functions of a teleco­mmu­nic­ation or computing system without regard of their underlying internal structure and techno­logy.
        - A layer serves the layer above it and is served by the layer below it.
        - Encapsulation
          - Preparing & passing the data by any Upper layer to the layer below it.
          - Going from the application layer all the way down to the physical layer.
        - Decapsulation
          - Decoding data while going Upwards from the physical layer till application layer.
        - Abstra­ction layers
          - Physical Layer
            - Transm­ission and reception of raw bit streams over a physical medium.
            - Specifies electrical, mechanical, procedural and functional requirements for activating, maintaining and deactivating a physical link.
          - Data Link Layer
            - Reliable transm­ission of data frames between two nodes connected by a physical layer.
            - Provides physical transmission of data, handles error notification, flow control and network topology. Split into two sub layers (LLC and MAC)
          - Network Layer
            - Struct­uring and managing a multi-node network, including addres­sing, routing and traffic control.
            - Manages logical addressing and path determination.
          - Transport Layer
            - Reliable transm­ission of data segments between points on a network, including segmen­tation, acknow­led­gement and multip­lexing.
            - Provides end-to-end transport services - establishes logical connections between hosts. Connection-oriented or connectionless data transfer.
          - Session Layer
            - Managing commun­ication sessions, i.e. continuous exchange of inform­ation in the form of multiple back-a­nd-­forth transm­issions between two nodes.
            - Setting up, managing and tearing down sessions. Keeps application’s data separate.
          - Presentation Layer
            - Transl­ation of data between a networking service and an applic­ation; including character encoding, data compre­ssion and encryp­tio­n/d­ecr­yption.
            -  Data translation, encryption, code formatting .
          - Application Layer
            - High-level APIs, including resource sharing, remote file access, directory services and virtual terminals.
            - Identifying and establishing the availability of intended communication partner and whether there are sufficient resources.
      - TCP/IP Model
        - TCP/IP provides end-to-end connec­tivity specifying how data should be packet­ized, addressed, transm­itted, routed and received at the destin­ation.
        - This functi­onality is organized into four abstra­ction layers which are used to sort all related protocols according to the scope of networking involved.
        - Abstra­ction layers
          - Network Access Layer
            - Specifies how data is physically sent through the network, including how bits are electr­ically signaled by hardware devices that interface directly with a network medium, such as: Coaxial cable, Optical fiber, Twiste­d-pair copper wire, Ethernet, RS-232
          - Internet Layer
            - Packages data into IP datagrams, containing source and destin­ation address inform­ation used to forward datagrams between hosts and across networks.
            - Protocols are: IP (Internet Protocol), ICMP (I­nternet Control Message Protocol), ARP (Address Resolution Protocol), RARP (Reverse Address Resolution Protocol)
          - Host-to-Host Layer
            - Provides commun­ication session management between computers. Defines the level of service and status of the connec­tio­n.
            - The main protocols included at Transport layer are: TCP (Trans­mission Control Protocol) and UDP (User Datagram Protocol).
          - Application Layer
            - Applic­ation layer includes all the higher­-level protocols:
              - Telnet - Terminal emulation program - port 23
              - TFTP - UDP file transfer - port 69
              - SMTP - Simple Mail Transfer Protocol - port 25
              - SSH - Secure Shell - port 22
              - FTP - TCP file transfer service - port 20-21
              - DNS - Domain Naming System - Resolves FQDNs to IP addresses - port 53
              - RDP - Remote Desktop Protocol
              - HTTP - Hypertext Transfer Protocol
              - DHCP - Dynamic Host Config­uration Protocol - Assigns IP addresses to hosts - ports 67 and 68
              - SNMP - Simple Network Management Protocol
    - Fallacies of distributed computing
      - The network is reliable.
        - Software applications are written with little error-handling on networking errors.
        - During a network outage, such applications may stall or infinitely wait for an answer packet, permanently consuming memory or other resources.
        - When the failed network becomes available, those applications may also fail to retry any stalled operations or require a (manual) restart.
      - Latency is zero.
        - Ignorance of network latency, and of the packet loss it can cause, induces application- and transport-layer developers to allow unbounded traffic, greatly increasing dropped packets and wasting bandwidth.
      - Bandwidth is infinite.
        - Ignorance of bandwidth limits on the part of traffic senders can result in bottlenecks.
      - The network is secure.
        - Complacency regarding network security results in being blindsided by malicious users and programs that continually adapt to security measures.[2]
      - Topology doesn't change.
        - Changes in network topology can have effects on both bandwidth and latency issues, and therefore can have similar problems.
      - There is one administrator.
        - Multiple administrators, as with subnets for rival companies, may institute conflicting policies of which senders of network traffic must be aware in order to complete their desired paths.
      - Transport cost is zero.
        - The "hidden" costs of building and maintaining a network or subnet are non-negligible and must consequently be noted in budgets to avoid vast shortfalls.
      - The network is homogeneous.
        - If a system assumes a homogeneous network, then it can lead to the same problems that result from the first three fallacies.
    - CAP theorem
      - Consistency
        - Every read receives the most recent write or an error.
      - Availability
        - Every request receives a (non-error) response - without the guarantee that it contains the most recent write.
      - Partition tolerance
        - The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.
    - Benefits of Serverless Architecture
      - Lower costs and scalability.
      - Faster development and deployment.
      - Reduced expenses on human resources.
      - High availability and auto-scaling.
      - Focus on business needs.
    - The Twelve Factors
      - Codebase
        - One codebase tracked in revision control, many deploys.
      - Dependencies
        - Explicitly declare and isolate dependencies.
      - Config
        - Store config in the environment.
      - Backing services
        - Treat backing services as attached resources.
      - Build, release, run
        - Strictly separate build and run stages.
      - Processes
        - Execute the app as one or more stateless processes.
      - Port binding
        - Export services via port binding.
      - Concurrency
        - Scale out via the process model.
      - Disposability
        - Maximize robustness with fast startup and graceful shutdown.
      - Dev/prod parity
        - Keep development, staging, and production as similar as possible.
      - Logs
        - Treat logs as event streams.
      - Admin processes
        - Run admin/management tasks as one-off processes.
    - Desirable qualities of a build process
      - Correct
        - never need to "clean" or wonder if build outputs are up-to-date
      - Fast
        - provides fast build execution
      - Parallelism
        - execute on multiple local or remote CPUs
      - Incremental
        - don't re-build everything from scratch
      - Deterministic
        - the output depends entirely on the inputs (predictability, caching)
      - Composable
        - allows reuse of existing build rules and creating new ones by combining them
      - Universal
        - Builds Android, iOS, web, backends, cloud services, and more
      - Multi-process
        - Isolation, allow more tool runtimes, defense against mem leaks
  - Clean Code
    - Why Clean Code
      - Code is clean if it can be understood easily - by everyone on the team.
      - With understandability comes readability, changeability, extensibility and maintainability.
      - All the things needed to keep a project going over a long time without accumulating up a large amount of technical debt.
      - Writing clean code from the start in a project is an investment in keeping the cost of change as constant as possible throughout the lifecycle of a software product.
      - Therefore, the initial cost of change is a bit higher when writing clean code than quick and dirty programming, but is paid back quite soon.
      - Especially if you keep in mind that most of the cost has to be paid during maintenance of the software.
      - Unclean code results in technical debt that increases over time if not refactored into clean code.
      - There are other reasons leading to Technical Debt such as bad processes and lack of documentation, but unclean code is a major driver.
      - As a result, your ability to respond to changes is reduced.
      - In Clean Code, Bugs Cannot Hide
        - Most software defects are introduced when changing existing code.
        - The reason behind this is that the developer changing the code cannot fully grasp the effects of the changes made.
        - Clean code minimises the risk of introducing defects by making the code as easy to understand as possible.
    - Principles
      - (+) Loose Coupling
        - Two classes, components or modules are coupled when at least one of them uses the other.
        - The less these items know about each other, the looser they are coupled.
        - A component that is only loosely coupled to its environment can be more easily changed or replaced than a strongly coupled component.
      - (+) High Cohesion
         - Cohesion is the degree to which elements of a whole belong together.
         - Methods and fields in a single class and classes of a component should have high cohesion.
         - High cohesion in classes and components results in simpler, more easily understandable code structure and design.
      - (+) Change is Local
        - When a software system has to be maintained, extended and changed for a long time, keeping change local reduces involved costs and risks.
        - Keeping change local means that there are boundaries in the design which changes do not cross.
      - (+) It is Easy to Remove
        - We normally build software by adding, extending or changing features.
        - However, removing elements is important so that the overall design can be kept as simple as possible.
        - When a block gets too complicated, it has to be removed and replaced with one or more simpler blocks.
      - (+) Mind-sized Components
        - Break your system down into components that are of a size you can grasp within your mind so that you can predict consequences of changes easily (dependencies, control flow, …).
    - Smells
      - (-) Rigidity
        - The software is difficult to change. A small change causes a cascade of subsequent changes.
      - (-) Fragility
        - The software breaks in many places due to a single change.
      - (-) Immobility
        - You cannot reuse parts of the code in other projects because of involved risks and high effort.
      - (-) Viscosity of Design
        - Taking a shortcut and introducing technical debt requires less effort than doing it right.
      - (-) Viscosity of Environment
        - Building, testing and other tasks take a long time. Therefore, these activities are not executed properly by everyone and technical debt is introduced.
      - (-) Needless Complexity
        - The design contains elements that are currently not useful. The added complexity makes the code harder to comprehend. Therefore, extending and changing the code results in higher effort than necessary.
      - (-) Needless Repetition
        - Code contains exact code duplications or design duplicates (doing the same thing in a different way).
        - Making a change to a duplicated piece of code is more expensive and more error-prone because the change has to be made in several places with the risk that one place is not changed accordingly.
      - (-) Opacity
        - The code is hard to understand. Therefore, any change takes additional time to first reengineer the code and is more likely to result in defects due to not understanding the side effects.
    - Class Design
      - (+) Single Responsibility Principle (SRP)
        - A class should have one, and only one, reason to change.
      - (+) Open Closed Principle (OCP)
        - You should be able to extend a classes behaviour without modifying it.
      - (+) Liskov Substitution Principle (LSP)
        - Derived classes must be substitutable for their base classes.
      - (+) Dependency Inversion Principle (DIP)
        - Depend on abstractions, not on concretions.
      - (+) Interface Segregation Principle (ISP)
        - Make fine grained interfaces that are client-specific.
      - (+) Classes Should be Small
        - Smaller classes are easier to grasp. Classes should be smaller than about 100 lines of code. Otherwise, it is hard to spot how the class does its job and it probably does more than a single job.
      - (+) Do stuff or know others, but not both
        - Classes should either do stuff (algorithm, read data, write data, …) or orchestrate other classes. This reduces coupling and simplifies testing
    - Package Cohesion
      - (+) Release Reuse Equivalency Principle (RREP)
        - The granule of reuse is the granule of release.
      - (+) Common Closure Principle (CCP)
        - Classes that change together are packaged together.
      - (+) Common Reuse Principle (CRP)
        - Classes that are used together are packaged together.
    - Package Coupling
      - (+) Acyclic Dependencies Principle (ADP)
        - The dependency graph of packages must have no cycles.
      - (+) Stable Dependencies Principle (SDP)
        - Depend in the direction of stability.
      - (+) Stable Abstractions Principle (SAP)
        - Abstractness increases with stability.
    - General
      - (+) Follow Standard Conventions
        - Coding-, architecture-, design guidelines (check them with tools)
      - (+) Keep it Simple, Stupid (KISS)
        - Simpler is always better. Reduce complexity as much as possible.
      - (+) Boy Scout Rule
        - Leave the campground cleaner than you found it.
      - (+) Root Cause Analysis
        - Always look for the root cause of a problem. Otherwise, it will get you again.
      - (-) Multiple Languages in One Source File
        - C#, Java, JavaScript, XML, HTML, XAML, English, German …
    - Environment
      - (+) Project Build Requires Only One Step
        - Check out and then build with a single command.
      - (+) Executing Tests Requires Only One Step
        - Run all unit tests with a single command.
      - (+) Source Control System
        - Always use a source control system.
      - (+) Continuous Integration
        - Assure integrity with Continuous Integration
      - (-) Overridden Safeties
        - Do not override warnings, errors, exception handling - they will catch you.
    - Dependency Injection
      - (+) Decouple Construction from Runtime
        - Decoupling the construction phase completely from the runtime helps to simplify the runtime behaviour.
    - Design
      - (+) Keep Configurable Data at High Levels
        - If you have a constant such as default or configuration value that is known and expected at a high level of abstraction, do not bury it in a low-level function.
        - Expose it as an argument to the low-level function called from the high-level function.
      - (+) Don’t Be Arbitrary
        - Have a reason for the way you structure your code, and make sure that reason is communicated by the structure of the code.
        - If a structure appears arbitrary, others will feel empowered to change it.
      - (+) Be Precise
        - When you make a decision in your code, make sure you make it precisely.
        - Know why you have made it and how you will deal with any exceptions.
      - (+) Structure over Convention
        - Enforce design decisions with structure over convention.
        - Naming conventions are good, but they are inferior to structures that force compliance.
      - (+) Prefer Polymorphism To If/Else or Switch/Case
        - “ONE SWITCH”: There may be no more than one switch statement for a given type of selection.
        - The cases in that switch statement must create polymorphic objects that take the place of other such switch statements in the rest of the system.
      - (+) Symmetry / Analogy
        - Favour symmetric designs (e.g. Load - Save) and designs that follow analogies (e.g. same design as found in .NET framework).
      - (+) Separate Multi-Threading Code
        - Do not mix code that handles multi-threading aspects with the rest of the code. Separate them into different classes
      - (-) Misplaced Responsibility
        - Something put in the wrong place.
      - (-) Code at Wrong Level of Abstraction
        - Functionality is at wrong level of abstraction, e.g. a PercentageFull property on a generic IStack<T>.
      - (-) Fields Not Defining State
        - Fields holding data that does not belong to the state of the instance but are used to hold temporary data. Use local variables or extract to a class abstracting the performed action.
      - (-) Over Configurability
        - Prevent configuration just for the sake of it - or because nobody can decide how it should be. Otherwise, this will result in overly complex, unstable systems.
      - (-) Micro Layers
        - Do not add functionality on top, but simplify overall.
    - Dependencies
      - (+) Make Logical Dependencies Physical
        - If one module depends upon another, that dependency should be physical, not just logical. Don’t make assumptions.
      - (-) Singletons / Service Locator
         - Use dependency injection. Singletons hide dependencies.
      - (-) Base Classes Depending On Their Derivatives
        - Base classes should work with any derived class.
      - (-) Too Much Information
        - Minimise interface to minimise coupling
      - (-) Feature Envy
        - The methods of a class should be interested in the variables and functions of the class they belong to, and not the variables and functions of other classes.
        - Using accessors and mutators of some other object to manipulate its data, is envying the scope of the other object.
      - (-) Artificial Coupling
        - Things that don’t depend upon each other should not be artificially coupled.
      - (-) Hidden Temporal Coupling
        - If, for example, the order of some method calls is important, then make sure that they cannot be called in the wrong order.
      - (-) Transitive Navigation
        - Aka Law of Demeter, writing shy code.
        - A module should know only its direct dependencies.
    - Naming
      - (+) Choose Descriptive / Unambiguous Names
        - Names have to reflect what a variable, field, property stands for. Names have to be precise.
      - (+) Choose Names at Appropriate Level of Abstraction
        - Choose names that reflect the level of abstraction of the class or method you are working in.
      - (+) Name Interfaces After Functionality They Abstract
        - The name of an interface should be derived from its usage by the client.
      - (+) Name Classes After How They Implement Interfaces
        - The name of a class should reflect how it fulfils the functionality provided by its interface(s), such as MemoryStream : IStream 
      - (+) Name Methods After What They Do
        - The name of a method should describe what is done, not how it is done.
      - (+) Use Long Names for Long Scopes
        - (long) fields -> parameters -> locals -> loop variables (short)
      - (+) Names Describe Side Effects
        - Names have to reflect the entire functionality.
      - (+) Standard Nomenclature Where Possible
        - Don’t invent your own language when there is a standard.
      - (-) Encodings in Names
        - No prefixes, no type/scope information
    - Understandability
      - (+) Consistency
        - If you do something a certain way, do all similar things in the same way: same variable name for same concepts, same naming pattern for corresponding concepts.
      - (+) Use Explanatory Variables
        - Use locals to give steps in algorithms names.
      - (+) Encapsulate Boundary Conditions
        - Boundary conditions are hard to keep track of. Put the processing for them in one place, e.g. nextLevel = level + 1;
      - (+) Prefer Dedicated Value Objects to Primitive Types
        - Instead of passing primitive types like strings and integers, use dedicated primitive types: e.g. AbsolutePath instead of string.
      - (-) Poorly Written Comment
        - Comment does not add any value (redundant to code), is not well formed, not correct grammar/spelling.
      - (-) Obscured Intent
        - Too dense algorithms that lose all expressiveness.
      - (-) Obvious Behaviour Is Unimplemented
        - Violations of “the Principle of Least Astonishment”. What you expect is what you get.
      - (-) Hidden Logical Dependency
        - A method can only work when invoked correctly depending on something else in the same class, e.g. a DeleteItem method must only be called if a CanDeleteItem method returned true, otherwise it will fail
    - Methods
      - (+) Methods Should Do One Thing
        - Loops, exception handling, … encapsulate in sub-methods.
      - (+) Methods Should Descend 1 Level of Abstraction
        - The statements within a method should all be written at the same level of abstraction, which should be one level below the operation described by the name of the function.
      - (-) Method with Too Many Arguments
        - Prefer fewer arguments. Maybe functionality can be outsourced to a dedicated class that holds the information in fields.
      - (-) Method with Out/Ref Arguments
        - Prevent usage. Return complex object holding all values, split into several methods. If your method must change the state of something, have it change the state of the object it is called on.
      - (-) Selector / Flag Arguments
        - public int Foo(bool flag)
        - Split method into several independent methods that can be called from the client without the flag.
      - (-) Inappropriate Static
        - Static method that should be an instance method
    - Source Code Structure
      - (+) Vertical Separation
        - Variables and methods should be defined close to where they are used.
        - Local variables should be declared just above their first usage and should have a small vertical scope.
      - (+) Nesting
        - Nested code should be more specific or handle less probable scenarios than unnested code.
      - (+) Structure Code into Namespaces by Feature
        - Keep everything belonging to the same feature together. Don't use namespaces communicating layers. A feature may use another feature; a business feature may use a core feature like logging.
    - Conditionals
      - (+) Encapsulate Conditionals
        - if (this.ShouldBeDeleted(timer)) is preferable to if (timer.HasExpired && !timer.IsRecurrent)
      - (+) Positive Conditionals
        - Positive conditionals are easier to read than negative conditionals.
    - Useless Stuff
      - Dead Comment, Code
        - (-) Delete unused things. You can find them in your version control system.
      - Clutter
        - (-) Code that is not dead but does not add any functionality
      - Inappropriate Information
        - (-) Comment holding information better held in a different system: product backlog, source control. Use code comments for technical notes only.
    - Maintainability Killers
      - (+) Duplication
        - Eliminate duplication. Violation of the “Don’t repeat yourself” (DRY) principle.
      - (+) Magic Numbers / Strings
        - Replace Magic Numbers and Strings with named constants to give them a meaningful name when meaning cannot be derived from the value itself
      - (+) Enums (Persistent or Defining Behaviour)
        - Use reference codes instead of enums if they have to be persisted. Use polymorphism instead of enums if they define behaviour.
      - (+) Tangles
        - The class dependencies should not be tangled. There should be no cyclic dependency chains. In a cycle there is no point to start changing the code without side-effects.
    - Exception Handling
      - (+) Catch Specific Exceptions
        - Catch exceptions as specific as possible. Catch only the exceptions for which you can react in a meaningful manner.
      - (+) Catch Where You Can React in a Meaningful Way
        - Only catch exceptions when you can react in a meaningful way. Otherwise, let someone up in the call stack react to it.
      - (+) Use Exceptions instead of Return Codes or null
        - In an exceptional case, throw an exception when your method cannot do its job. Don't accept or return null. Don't return error codes.
      - (+) Fail Fast
        - Exceptions should be thrown as early as possible after detecting an exceptional case. This helps to pinpoint the exact location of the problem by looking at the stack trace of the exception.
      - (-) Using Exceptions for Control Flow
        - Using exceptions for control flow: has bad performance, is hard to understand and results in very hard handling of real exceptional cases.
      - (-) Swallowing Exceptions
        - Exceptions can be swallowed only if the exceptional case is completely resolved after leaving the catch block. Otherwise, the system is left in an inconsistent state.
    - From Legacy Code to Clean Code
      - (+) Always have a Running System
        - Change your system in small steps, from a running state to a running state. 
      - (+) Identify Features
        - Identify the existing features in your code and prioritise them according to how relevant they are for future development (likelihood and risk of change).
      - (+) Introduce Boundary Interfaces for Testability
        - Refactor the boundaries of your system to interfaces so that you can simulate the environment with test doubles (fakes, mocks, stubs).
      - (+) Write Feature Acceptance Tests
        - Cover a feature with Acceptance Tests to establish a safety net for refactoring.
      - (+) Identify Components
        - Within a feature, identify the components used to provide the feature.
        - Prioritise components according to relevance for future development (likelihood and risk of change).
      - (+) Refactor Interfaces between Components
        - Refactor (or introduce) interfaces between components so that each component can be tested in isolation of its environment.
      - (+) Write Component Acceptance Tests
        - Cover the features provided by a component with Acceptance Tests.
      - (+) Decide for Each Component: 
        - Refactor, Reengineer, Keep
        - Decide for each component whether to refactor, reengineer or keep it.
      - (+) Refactor Component
        - Redesign classes within the component and refactor step by step (see Refactoring Patters). Add unit tests for each newly designed class.
      - (+) Reengineer Component
        - Use ATDD and TDD (see Clean ATDD/TDD cheat sheet) to re-implement the component.
      - (+) Keep Component
        - If you anticipate only few future changes to a component and the component had few defects in the past, consider keeping it as it is.
    - Refactoring Patterns
      - (+) Reconcile Differences - Unify Similar Code
        - Change both pieces of code stepwise until they are identical. Then extract.
      - (+) Isolate Change
        - First, isolate the code to be refactored from the rest. Then refactor. Finally, undo isolation.
      - (+) Migrate Data
        - Move from one representation to another by temporary duplication of data structures.
      - (+) Temporary Parallel Implementation
        - Refactor by introducing a temporary parallel implementation of an algorithm. Switch one caller after the other. Remove old solution when no longer needed. This way you can refactor with only one red test at a time.
      - (+) Demilitarized Zone for Components
        - Introduce an internal component boundary and push everything unwanted outside of the internal boundary into the demilitarized zone between component interface and internal boundary. Then refactor the component interface to match the internal boundary and eliminate the demilitarized zone.
      - (+) Refactor before adding Functionality
        - Refactor the existing code before adding new functionality in a way so that the change can easily be made.
      - (+) Small Refactorings
        - Only refactor in small steps with working code in-between so that you can keep all loose ends in your head. Otherwise, defects sneak in.
    - How to Learn Clean Code
      - (+) Pair Programming
        - Two developers solving a problem together at a single workstation. One is the driver, the other is the navigator. The driver is responsible for writing the code. The navigator is responsible for keeping the solution aligned with the architecture, the coding guidelines and looks at where to go next (e.g. which test to write next). Both challenge their ideas and approaches to solutions.
      - (+) Commit Reviews
        - A developer walks a peer developer through all code changes prior to committing (or pushing) the changes to the version control system. The peer developer checks the code against clean code guidelines and design guidelines.
      - (+) Coding Dojo
        - In a Coding Dojo, a group of developers come together to exercise their skills. Two developers solve a problem (kata) in pair programming. The rest observe. After 10 minutes, the group rotates to build a new pair. The observers may critique the current solution, but only when all tests are green.
    - Kinds of Automated Tests
      - ATDD - Acceptance Test Driven Development
        - Specify a feature first with a test, then implement.
      - TDD - Test Driven Development
        - Red - green - refactor. Test a little - code a little.
      - DDT - Defect Driven Testing
        - Write a unit test that reproduces the defect - Fix code - Test will succeed - Defect will never return.
      - POUTing - Plain Old Unit Testing
        - Aka test after. Write unit tests to check existing code. You cannot and probably do not want to test drive everything. Use POUT to increase sanity. Use to add additional tests after TDDing (e.g. boundary cases).
    - Design for Testability
      - Constructor - Simplicity
        - Objects have to be easily creatable. Otherwise, easy and fast testing is not possible.
      - Constructor - Lifetime
        - Pass dependencies and configuration/parameters into the constructor that have a lifetime equal to or longer than the created object. For other values use methods or properties
      - Abstraction Layers at System Boundary
        - Use abstraction layers at system boundaries (database, file system, web services, ...) that simplify unit testing by enabling the usage of fakes.
    - Structure
      - Arrange - Act - Assert
        - Structure the tests always by AAA. Never mix these three blocks.
      - Test Assemblies (.Net)
        - Create a test assembly for each production assembly and name it as the production assembly + “.Test”/”.Facts”/… .
      - Test Namespace
        - Put the tests in the same namespace as their associated testee.
      - Unit Test Methods Show Whole Truth
        - Unit test methods show all parts needed for the test. Do not use SetUp method or base classes to perform actions on testee or dependencies.
      - SetUp / TearDown for Infrastructure Only
        - Use the SetUp / TearDown methods only for infrastructure that your unit test needs. Do not use it for anything that is under test.
      - Test Method Naming
        - Use a pattern that reflects behaviour of tested code, e.g. Behaviour[_OnTrigger][_WhenScenario] with [] as optional parts.
      - Resource Files 
        - Test and resource are together: FooTest.cs, FooTest.resx
    - Naming
      - Naming SUT Test Variables
        - Give the variable holding the System Under Test always the same name (e.g. testee or sut). Clearly identifies the SUT, robust against refactoring.
      - Naming Result Values
        - Give the variable holding the result of the tested method always the same name (e.g. result).
      - Anonymous Variables
        - Always use the same name for variables holding uninteresting arguments to tested methods (e.g. anonymousText, anyText).
    - Don’t Assume
      - (+) Understand the Algorithm
        - Just working is not enough, make sure you understand why it works.
      - (-) Incorrect Behaviour at Boundaries
        - Always unit test boundaries. Do not assume behaviour. 
    - Faking (Stubs, Fakes, Spies, Mocks, Test Doubles …)
      - (+) Isolation from environment
        - Use fakes to simulate all dependencies of the testee.
      - (+) Faking Framework
        - Use a dynamic fake framework for fakes that show different behaviour in different test scenarios (little behaviour reuse).
      - (+) Manually Written Fakes
        - Use manually written fakes when they can be used in several tests and they have only little changed behaviour in these scenarios (behaviour reuse).
      - (-) Mixing Stubbing and Expectation Declaration
        - Make sure that you follow the AAA (arrange, act, assert) syntax when using fakes. Don’t mix setting up stubs (so that the testee can run) with expectations (on what the testee should do) in the same code block.
      - (-) Checking Fakes instead of Testee
        - Tests that do not check the testee but values returned by fakes. Normally due to excessive fake usage.
      - (-) Excessive Fake Usage
        - If your test needs a lot of fakes or fake setup, then consider splitting the testee into several classes or provide an additional abstraction between your testee and its dependencies.
    - Unit Test Principles
      - (+) Fast
        - Unit tests have to be fast in order to be executed often. Fast means much smaller than seconds.
      - (+) Isolated
        - Isolated testee: Clear where the failure happened.
        - Isolated test: No dependency between tests (random order).
      - (+) Repeatable
        - No assumed initial state, nothing left behind, no dependency on external services that might be unavailable (databases, file system …).
      - (+) Self-Validating
        - No manual test interpretation or intervention. Red or green!
      - (+) Timely
        - Tests are written at the right time (TDD, DDT, POUTing)
    - Unit Test Smells
      - (-) Test Not Testing Anything
        - Passing test that at first sight appears valid but does not test the testee.
      - (-) Test Needing Excessive Setup
        - A test that needs dozens of lines of code to set up its environment. This noise makes it difficult to see what is really tested.
      - (-) Too Large Test / Assertions for Multiple Scenarios
        - A valid test that is, however, too large. Reasons can be that this test checks for more than one feature or the testee does more than one thing (violation of Single Responsibility Principle).
      - (-) Checking Internals
        - A test that accesses internals (private/protected members) of the testee directly (Reflection). This is a refactoring killer.
      - (-) Test Only Running on Developer’s Machine
        - A test that is dependent on the development environment and fails elsewhere. Use continuous integration to catch them as soon as possible.
      - (-) Test Checking More than Necessary
        - A test that checks more than it is dedicated to. The test fails whenever something changes that it checks unnecessarily. Especially probable when fakes are involved or checking for item order in unordered collections. 
      - (-) Irrelevant Information
        - Test contains information that is not relevant to understand it.
      - (-) Chatty Test
        - A test that fills the console with text - probably used once to manually check for something.
      - (-) Test Swallowing Exceptions
        - A test that catches exceptions and lets the test pass.
      - (-) Test Not Belonging in Host Test Fixture
        - A test that tests a completely different testee than all other tests in the fixture.
      - (-) Obsolete Test
        - A test that checks something no longer required in the system. May even prevent clean-up of production code because it is still referenced.
      - (-) Hidden Test Functionality
        - Test functionality hidden in either the SetUp method, base class or helper class. The test should be clear by looking at the test method only - no initialisation or asserts somewhere else.
      - (-) Bloated Construction
        - The construction of dependencies and arguments used in calls to testee makes test hardly readable. Extract to helper methods that can be reused.
      - (-) Unclear Fail Reason
        - Split test or use assertion messages.
      - (-) Conditional Test Logic
        - Tests should not have any conditional test logic because it’s hard to read.
      - (-) Test Logic in Production Code
        - Tests depend on special logic in production code.
      - (-) Erratic Test
        - Sometimes passes, sometimes fails due to left overs or environment.
    - TDD Principles
      - (+) A Test Checks One Feature
        - A test checks exactly one feature of the testee. That means that it tests all things included in this feature but not more. This includes probably more than one call to the testee. This way, the tests serve as samples and documentation of the usage of the testee.
      - (+) Tiny Steps
        - Make tiny little steps. Add only a little code in test before writing the required production code. Then repeat. Add only one Assert per step.
      - (+) Keep Tests Simple
        - Whenever a test gets complicated, check whether you can split the testee into several classes (Single Responsibility Principle)
      - (+) Prefer State Verification to Behaviour Verification
        - Use behaviour verification only if there is no state to verify. Refactoring is easier due to less coupling to implementation.
      - (+) Test Domain Specific Language
        - Use test DSLs to simplify reading tests: builders to create test data using fluent APIs, assertion helpers for concise assertions.
    - TDD Process Smells
      - (-) Using Code Coverage as a Goal
        - Pick a test you are confident you can implement and which maximises learning effect (e.g. impact on design).
      - (-) No Green Bar in the last ~10 Minutes
        - Make small steps to get feedback as fast and frequent as possible.
      - (-) Not Running Test Before Writing Production Code
        - Only if the test fails, then new code is required. Additionally, if the test surprisingly does not fail then make sure the test is correct.
      - (-) Not Spending Enough Time on Refactoring
        - Refactoring is an investment in the future. Readability, changeability and extensibility will pay back.
      - (-) Skipping Something Too Easy to Test
        - Don’t assume, check it. If it is easy, then the test is even easier.
      - (-) Skipping Something Too Hard to Test
        - Make it simpler, otherwise bugs will hide in there and maintainability will suffer.
      - (-) Organising Tests around Methods, Not Behaviour
        - These tests are brittle and refactoring killers. Test complete “mini” use cases in a way which reflects how the feature will be used in the real world. Do not test setters and getters in isolation, test the scenario they are used in.
    - Red Bar Patterns
      - (+) One Step Test
        - Pick a test you are confident you can implement and which maximises learning effect (e.g. impact on design).
      - (+) Partial Test
        - Write a test that does not fully check the required behaviour, but brings you a step closer to it. Then use Extend Test below.
      - (+) Extend Test
        - Extend an existing test to better match real-world scenarios.
      - (+) Another Test
        - If you think of new tests, then write them on the TO DO list and don’t lose focus on current test.
      - (+) Learning Test
        - Write tests against external components to make sure they behave as expected.
    - Green Bar Patterns
      - (+) Fake It (‘Til You Make It)
        - Return a constant to get first test running. Refactor later.
      - (+) Triangulate - Drive Abstraction
        - Write test with at least two sets of sample data. Abstract implementation on these.
      - (+) Obvious Implementation
        - If the implementation is obvious then just implement it and see if test runs. If not, then step back and just get test running and refactor then.
      - (+) One to Many - Drive Collection Operations
        - First, implement operation for a single element. Then, step to several elements (and no element).
    - Acceptance Test Driven Development
      - (+) Use Acceptance Tests to Drive Your TDD tests
        - Acceptance tests check for the required functionality. Let them guide your TDD.
      - (+) User Feature Test
        - An acceptance test is a test for a complete user feature from top to bottom that provides business value.
      - (+) Automated ATDD
        - Use automated Acceptance Test Driven Development for regression testing and executable specifications.
      - (+) Component Acceptance Tests
        - Write acceptance tests for individual components or subsystems so that these parts can be combined freely without losing test coverage.
      - (+) Simulate System Boundaries
        - Simulate system boundaries like the user interface, databases, file system and external services to speed up your acceptance tests and to be able to check exceptional cases (e.g. a full hard disk). Use system tests to check the boundaries.
      - (-) Acceptance Test Spree
        - Do not write acceptance tests for every possibility. Write acceptance tests only for real scenarios. The exceptional and theoretical cases can be covered more easily with unit tests.
    - Continuous Integration
      - (+) Pre-Commit Check
        - Run all unit and acceptance tests covering currently worked on code prior to committing to the source code repository.
      - (+) Post-Commit Check
        - Run all unit and acceptance tests on every commit to the version control system on the continuous integration server.
      - (+) Communicate Failed Integration to Whole Team
        - Whenever a stage on the continuous integration server fails, notify whole team in order to get blocking situation resolved as soon as possible.
      - (+) Build Staging
        - Split the complete continuous integration workflow into individual stages to reduce feedback time.
      - (+) Automatically Build an Installer for Test System
        - Automatically build an installer as often as possible to test software on a test system (for manual tests, or tests with real hardware).
      - (+) Continuous Deployment
        - Install the system to a test environment on every commit/push and on manual request. Deployment to production environment is automated to prevent manual mistakes, too.
  - Security principles according to OWASP
    - Minimise attack surface area
    - Establish secure defaults
    - The principle of Least privilege
    - The principle of Defence in depth
    - Fail securely
  - Cloud Native
    - The five elements of a cloud-native app
      - Application design
        - The move to microservices
          - Each microservice can be updated independently
          - Integration overhead is reduced
          - Erratic workloads can be handled much more easily
          - Testing is simplified
      - API exposure
        - Internal and external access via standardized methods
          - API versioning
          - Throttling
          - Circuit breakers
          - Data caching
      - Operational integration
        - Aggregating log and monitoring information to enable application management
          - Dynamic application topologies
          - Centralized logging and monitoring
          - Root cause analysis
      - DevOps
        - Automation across the application lifecycle
          - Assess the overall application lifecycle via a technique referred to as value chain mapping (VCM)
      - Testing
        - Changing the role and use of quality assurance (QA)
  - Testing
    - Test Types
      - Unit Tests
        - Testing of individual functions or classes by supplying input and making sure the output is as expected.
      - Integration Tests
        - Testing processes or components to behave as expected, including the side effects.
      - UI Tests (A.K.A Functional Tests)
        - Testing scenarios on the product itself, by controlling the browser or the website, regardless of the internal structure to ensure expected behavior.
    - Test Tools Types
      - Test launchers
      - Testing structure   
      - Assertions functions providers
      - Generate and display test progress and results.
      - Generate and compare snapshots of component and data structures to make sure changes from previous runs are intended
      - Provide mocks, spies, and stubs
      - Generate code coverage reports
      - Browser Controllers simulate user actions for Functional Tests. 
      - Visual Regression Tools are used to compare your site to its previous versions visually by using image comparison techniques.
  - Redux Three Principles
    - Single source of truth
      - The state of your whole application is stored in an object tree within a single store.
    - State is read-only
      - The only way to change the state is to emit an action, an object describing what happened.
    - Changes are made with pure functions
      - To specify how the state tree is transformed by actions, you write pure reducers.
  - Design Patterns
    - Creational Patterns
      - Abstract Factory
        - Provide an interface for creating families of related or dependent objects without specifying their concrete classes.
      - Builder
        - Separate the construction of a complex object from its representation so that the same construction process can create different representations.
      - Factory Method
        - Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses.
      - Prototype
        - Specify the kind of objects to create using a prototypical instance, and create new objects by copying this prototype.
      - Singleton
        - Ensure a class has only one instance and provide a global point of access to it.
    - Structural Patterns
      - Adapter
        - Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn't otherwise because of incompatible interfaces.
      - Bridge
          - Decouple an abstraction from its implementation so that the two can vary independently.
      - Composite
        - Compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly.
      - Decorator
          - Attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality.
      - Façade
        - Provide a unified interface to a set of interfaces in a subsystem. Façade defines a higher-level interface that makes the subsystem easier to use.
      - Flyweight
        - Use sharing to support large numbers of fine-grained objects efficiently.
      - Proxy
        - Provide a surrogate or placeholder for another object to control access to it.
    - Behavioral Patterns
      - Chain of Resp.
        - Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it.
      - Command
        - Encapsulate a request as an object, thereby letting you parameterize clients with different requests, queue or log requests, and support undoable operations.
      - Interpreter
        - Given a language, define a representation for its grammar along with an interpreter that uses the representation to interpret sentences in the language.
      - Iterator
        - Provide a way to access the elements of an aggregate object sequentially without exposing its underlying representation.
      - Mediator
        - Define an object that encapsulates how a set of objects interact. Mediator promotes loose coupling by keeping objects from referring to each other explicitly, and it lets you vary their interaction independently.
      - Memento
        - Without violating encapsulation, capture and externalize an object's internal state so that the object can be restored to this state later.
      - Observer
        - Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.
      - State
        - Allow an object to alter its behavior when its internal state changes. The object will appear to change its class.
      - Strategy
        - Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it.
      - Template Method
        - Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm's structure.
      - Visitor
        - Represent an operation to be performed on the elements of an object structure. Visitor lets you define a new operation without changing the classes of the elements on which it operates.
  - Unified Modeling Language
  - Functional Programming
    - Arity
      - The number of arguments a function takes.
      - From words like unary, binary, ternary, etc.
    - Higher-Order Functions (HOF)
      - A function which takes a function as an argument and/or returns a function.
    - Closure
      - A closure is a scope which retains variables available to a function when it's created.
      - This is important for partial application to work.
    - Partial Application
      - Partially applying a function means creating a new function by pre-filling some of the arguments to the original function.
      - Partial application helps create simpler functions from more complex ones by baking in data when you have it.
      - Curried functions are automatically partially applied.
    - Currying
      - The process of converting a function that takes multiple arguments into a function that takes them one at a time.
    - Auto Currying
    - Function Composition
    - Continuation
    - Purity
    - Side effects
    - Idempotent
    - Point-Free Style
    - Predicate
    - Contracts
    - Category
    - Value
    - Constant
    - Functor
    - Pointed Functor
    - Lift
    - Referential Transparency
    - Equational Reasoning
    - Lambda
    - Lambda Calculus
    - Lazy evaluation
    - Monoid
    - Monad
    - Comonad
    - Applicative Functor
    - Morphism
      - Endomorphism
      - Isomorphism
      - Homomorphism
      - Catamorphism
      - Anamorphism
      - Hylomorphism
      - Paramorphism
      - Apomorphism
    - Setoid
    - Semigroup
    - Foldable
    - Lens
    - Type Signatures
    - Algebraic data type
    - Sum type
    - Product type
    - Option
    - Function
    - Partial function
  - Algorithms
    - Sorting
    - Searching
  - Data Structures
    - Definitions
      - Node
        - an element containing data that make contain links to one or more parents/children May also be referred to as a vertex
      - Edge
        - a connection between two nodes
      - Root
        - The top node in a tree (a node without a parent)
      - Parent
        - A node connected to another node when moving towards the root
      - Child
        - a node connectd to another node when moving away from root
      - Descendant
        - a node reachable by repeated processing from parent to child
      - Ancestor
        - a node reachable by repeated processing from child to parent
      - Leaf
        - a node without any children
      - Degree
        - the number of sub trees of a node
      - Path
        - a sequence of nodes and edges connecting a node with a descendant
      - Depth
        - the depth of a node is the number of edges from the node to the tree’s root node
      - Subtree
        - of a tree T is a tree consisting of a node in T and all of it’s descendants in T
    - Concepts
      - Binary Tree
        - A binary tree is a tree data structure in which each node has at most two children.
          - Full Binary tree
            - every node in the tree has 0 or 2 children
          - Perfect binary tree
            - all interior nodes have two children and all leaves have the same depth or level
          - Complete binary tree
            - every level except possibly the last is completely filled, and all nodes in the last level are as far left as possible
      - Binary Search Tree (BST)
        - A binary search tree (BST) is a data structure that binary tree that keeps it’s keys in sorted order, so that operations can take advantage of the binary search principle (a logarithmic search that takes happens in O(log n) time).
      - B-Tree
        - A B-tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. It is a generalization of a binary search tree in that a node can have more than two children. A B-tree is optimized for systems that read and write large blocks of data. B-tree’s are commonly used in databases and file systems.
      - AVL Tree
        - An AVL Tree is a self-balancing binary search tree. The height of the two child subtrees of any node differ at most by one, otherwise the tree is re-balanced. Lookup, insertion, and deletion take O(log n) time. Insertions and deletion may cause a tree rotation.
      - Red-Black Tree
        - A red-black tree is a self-balancing binary search tree. Each node of the tree has an extra bit, which is interpreted as either black or red. The color bits are used to ensure the tree remains balanced during insertions and deletions. Operations occur in O(log n) time.
      - B+ Tree
        - A B+ tree is a B-tree in which each node only contains keys (not key-values), and to which an additional level is added at the bottom with linked leaves.
        - This makes for more efficient retrieval of data in block-oriented storage (once you find the start of the block, you can read sequentially without having to traverse up and down the tree to retrieve data nodes). Additionally, all leave nodes must be the same distance from the root node.
        - SQL Server & Oracle store table indexes in B+ trees, which are similar to B-trees, except that data is only stored in leaf nodes - all other nodes hold only key values and pointers to the next nodes.
      - Tree Traversal
        - Depth-first search
          - In a depth-first search, the search is deepened as much as possible on each child before going to the next sibling
          - Pre-Order
            - Display data of root
            - Traverse left subtree calling preorder function
            - Traverse right subtree calling preorder function
          - In-Order
              - Traverse left subtree calling preorder function
              - Display data of root
              - Traverse right subtree calling preorder function
            - An In-Order search will return the sorted contents of a BST (Binary Search Tree)
          - Post-Order
            - Traverse left subtree calling preorder function
            - Traverse right subtree calling preorder function
            - Display data of root
          - A stack can be used to perform a depth-first search
        - Breadth-first search
          - In a breadth-first search, all nodes on a level are visited before going to a lower level.
          - A Queue is often used to peform a breadth-first search
      - Tree Rotation
  - Software Architecture
    - Software Architecture Patterns
      - Layered Pattern
        - Split up code into “layers”, where each layer has a certain responsibility and provides a service to a higher layer.
        - Typical Layers
          - Presentation or UI layer
          - Application layer
          - Business or domain layer
          - Persistence or data access layer
          - Database layer
        - It provides an easy way of writing a well-organized and testable application.
        - It tends to lead to monolithic applications that are hard to split up afterward.
        - Developers often find themselves writing a lot of code to pass through the different layers, without adding any value in these layers. If all you are doing is writing a simple CRUD application, the layered pattern might be overkill for you.
      - Microkernel
        - Is useful when application has a core set of responsibilities and a collection of interchangeable parts on the side.
        - The microkernel will provide the entry point and the general flow of the application, without really knowing what the different plug-ins are doing.
        - This pattern provides great flexibility and extensibility.
        - Some implementations allow for adding plug-ins while the application is running.
        - Microkernel and plug-ins can be developed by separate teams.
        - It can be difficult to decide what belongs in the microkernel and what doesn’t.
        - The predefined API might not be a good fit for future plug-ins.
      - Command and Query Responsibility Segregation (CQRS)
        - Read and write operations that must be totally separated.
        - This also means that the model used for write operations (commands) will differ from the read models (queries).
        - Furthermore, the data will be stored in different locations.
        - In a relational database, this means there will be tables for the command model and tables for the read model.
        - Command models can focus on business logic and validation while read models can be tailored to specific scenarios.
        - You can avoid complex queries (e.g. joins in SQL) which makes the reads more performant.
        - Keeping the command and the read models in sync can become complex.
      - Event Sourcing
        - Store the current state of your model in the database, but rather the events that happened to the model.
        - Provide an audit log out of the box. Each event represents a manipulation of the data at a certain point in time.
        - This software architecture pattern can provide an audit log out of the box. Each event represents a manipulation of the data at a certain point in time.
      - Microservices
        - Writing multiple applications that will work together.
        - Each microservice has its own distinct responsibility and teams can develop them independently of other microservices.
        - The only dependency between them is the communication.
        - As microservices communicate with each other, you will have to make sure messages sent between them remain backwards-compatible.
        - You can write, maintain, and deploy each microservice separately.
        - With microservices, a lot of extra concerns come into play: communication, coordination, backward compatibility, logging, etc.
        - A microservices architecture should be easier to scale, as you can scale only the microservices that need to be scaled. There’s no need to scale the less frequently used pieces of the application.
        - It’s easier to rewrite pieces of the application because they’re smaller and less coupled to other parts.
        - A single action of a user can pass through multiple microservices. There are more points of failure, and when something does go wrong, it can take more time to pinpoint the problem.
        - Components
          - Discovery
          - Load-balancer
          - Resiliency
          - Metrics
          - Tracing
      - Serverless Functions
        - Short-lived Process
        - Event-driven Architecture
      - SaaS
        - Examples
          - API Gateway as a Service
          - Single Sign On (SSO) as a Service
          - Storage as a Service
          - DB as a Service
          - Cache as a Service
          - Notification as a Service
          - Messaging as as Service
  - Cloud Architecture
    - Cloud Computing Concepts
      - Patterns of this category describe workload experienced by applications, the service models employed by cloud providers, and the different deployment options for clouds.
      - Application Workloads
        - Patterns of this category describe workload experienced by applications. Workload measured in the form of application utilization, for example, the number of requests, server load etc. For each workload pattern it is discussed furthermore how an application experiencing this workload can benefit from cloud computing.
        - Static Workload
          - IT resources with an equal utilization over time experience Static Workload.
          - How can an equal utilization be characterized and how can applications experiencing this workload benefit from cloud computing?
          - Static Workloads are characterized by a more-or-less flat utilization profile over time within certain boundaries.
          - An application experiencing Static Workload is less likely to benefit from an elastic cloud that offers a pay-per-use billing, because the number of required resources is constant.
        - Periodic Workload
          - IT resources with a peaking utilization at reoccurring time intervals experience periodic workload.
          - How can a periodically peaking utilization over time be characterized and how can applications experiencing this workload benefit from cloud computing?
          - In our real lives, periodic tasks and routines are very common. For example, monthly paychecks, monthly telephone bills, yearly car checkups, weekly status reports, or the daily use of public transport during rush hour, all these tasks and routines occur in well-defined intervals.
          - From a customer perspective the cost-saving potential in scope of Periodic Workload is to use a provider with a pay-per-use pricing model allowing the decommissioning of resources during non-peak times.
        - Once-in-a-lifetime Workload
          - IT resources with an equal utilization over time disturbed by a strong peak occurring only once experience Once-in-a-lifetime Workload.
          - How can equal utilization with a one-time peak be characterized and how can applications experiencing this workload benefit from cloud computing?
          - As a special case of Periodic Workload, the peaks of periodic utilization can occur only once in a very long timeframe. Often, this peak is known in advance as it correlates to a certain event or task.
          - The elasticity of a cloud is used to obtain IT resources necessary. The provisioning and decommissioning of IT resources can often be realized as a manual task, because it is performed at a known point in time.
        - Unpredictable Workload
          - IT resources with a random and unforeseeable utilization over time experience unpredictable workload.
          - How can random and unforeseeable utilization be characterized and how can applications experiencing this workload benefit from cloud computing?
          - Random workloads are a generalization of Periodic Workloads as they require elasticity but are not predictable. Such workloads occur quite often in the real world.
          - Unplanned provisioning and decommissioning of IT resources is required. The necessary provisioning and decommissioning of IT resources is, therefore, automated to align the resource numbers to changing workload.
        - Continuously Changing Workload
          - IT resources with a utilization that grows or shrinks constantly over time experience Continuously Changing Workload.
          - How can a continuous growth or decline in utilization be characterized and how can applications experiencing this workload benefit from cloud computing?
          - Many applications experience a long term change in workload.
          - Continuously Changing Workload is characterized by an ongoing continuous growth or decline of the utilization. Elasticity of clouds enables applications experiencing Continuously Changing Workload to provision or decommission resources with the same rate as the workload changes.
      - Cloud Service Models
        - Patterns of this category describe different cloud types and how providers host the infrastructure to host them.
        - Infrastructure as a Service (IaaS)
          - Providers share physical and virtual hardware IT resources between customers to enable self-service, rapid elasticity, and pay-per-use pricing.
          - How can different customers share a physical hosting environment so that it can be used on-demand with a pay-per-use pricing model?
          - In the scope of Periodic Workloads with reoccurring peaks and the special case of Once-in-a-lifetime Workloads with one dramatic increase in workload, IT resources have to be provisioned flexibly.
          - A provider offers physical and virtual hardware, such as servers, storage and networking infrastructure that can be provisioned and decommissioned quickly through a self-service interface.
        - Platform as a Service (PaaS)
          - Providers share IT resources providing an application hosting environment between customers to enable self-service, rapid elasticity, and pay-per-use pricing.
          - How can custom applications of the same customer or different customers share an execution environment so that it can be used on-demand with a pay-per-use pricing model?
          - If many customers require similar hosting environments for their applications, there are many redundant installations resulting in an inefficient use of the overall cloud.
          - A cloud provider offers managed operating systems and middleware. Management operations are handled by the provider, such as the elastic scaling and failure resiliency of hosted applications.
        - Software as a Service (SaaS)
          - Providers share IT resources providing human-usable application software between customers to enable self-service, rapid elasticity, and pay-per-use pricing.
          - How can customers share a provider-supplied software application so that it can be used on-demand with a pay-per-use pricing model?
          - Small and medium enterprises may not have the manpower and know-how to develop custom software applications. Other applications have become commodity and are used by many companies, for example, office suites, collaboration software, or communications software.
          - A provider offers a complete software application to customers who may use it on-demand via a self-service interface.
      - Cloud Deployment Models
        - Patterns of this category compare and categorize different Cloud Service Models according to the layers of the application stack for which they provide IT resources.
        - Public Cloud
          - IT resources are provided as a service to a very large customer group in order to enable elastic use of a static resource pool.
          - How can the cloud properties - on demand self-service, broad network access, pay-per-use, resource pooling, and rapid elasticity - be provided to a large customer group?
          - A provider offering IT resources according to IaaS, PaaS, or SaaS has to maintain physical data centers. IT resources, nevertheless, shall be made accessible dynamically.
          - The hosting environment is shared between many customers possibly reducing the costs for an individual customer. Leveraging economies of scale enables a dynamic use of resources, because workload peaks of some customers occur during times of low workload of other customers.
        - Private Cloud
          - IT resources are provided as a service exclusively to one customer in order to meet high requirements on privacy, security, and trust while enabling elastic use of a static resource pool as good as possible.
          - How can the cloud properties - on demand self-service, broad network access, pay-per-use, resource pooling, and rapid elasticity - be provided in environments with high privacy, security and trust requirements?
          - Many factors, such as legal limitations, trust, and security regulations, motivate dedicated, company-internal hosting environments only accessible by employees and applications of a single company.
          - Cloud computing properties are enabled in a company-internal data center. Alternatively, the Private Cloud may be hosted exclusively in the data center of an external provider, then referred to as outsourced Private Cloud. Sometimes, Public Cloud providers also offer means to create an isolated portion of their cloud made accessible to only one customer: a virtual Private Cloud.
        - Community Cloud
          - IT resources are provided as a service to a group of customers trusting each other in order to enable collaborative elastic use of a static resource pool.
          - How can the cloud properties - on demand self-service, broad network access, pay-per-use, resource pooling, and rapid elasticity - be provided to exclusively to a group of customers forming a community of trust?
          - Whenever companies collaborate, they commonly have to access shared applications and data to do business. While these companies trust each other due to established contracts etc., the handled data and application functionality may be very sensitive and critical to their business.
          - IT resources required by all collaborating partners are offered in a controlled environment accessible only by the community of companies that generally trust each other.
        - Hybrid Cloud
          - Different clouds and static data centers are integrated to form a homogeneous hosting environment.
          - How can the cloud properties - on demand self-service, broad network access, pay-per-use, resource pooling, and rapid elasticity - be provided across clouds and other environments?
          - A company is likely to use a large set of applications to support its business, which have versatile requirements making different Cloud Deployment Models suitable to host them.
          - Private Clouds, Public Clouds, Community Clouds, and static data centers are integrated to deployed applications to the hosting environment best suited for their requirements while interconnection of these environments.
    - Cloud Offerings
      - Patterns of this category cover different functionality found in clouds regarding the functionality they provide to customers and the behavior they display.
      - Cloud Environments
        - Patterns of this category descibe the hosting environments of cloud in detail and refer to other offerings composed to form these environments.
        - Elastic Infrastructure
          - Hosting of virtual servers, disk storage, and configuration of network connectivity is offered via a self-service interface over a network.
          - How do Cloud Offerings providing infrastructure resources behave and how should they be used in applications?
          - An application experiences Periodic Workload, Once-in-a-lifetime Workload, Unpredictable Workload, or Continuously Changing Workload, the number of IT resources, such as servers, should be adjusted dynamically. In scope of the IaaS service model, the applications’ runtime infrastructure, thus, must support dynamic provisioning and decommissioning of virtual servers, disk storage and network connectivity.
          - An Elastic Infrastructure provides preconfigured virtual server images, storage and network connectivity that may be provisioned by customers using a self-service interface. Monitoring information is provided to inform about resource utilization required for traceable billing and automation of management tasks.
        - Elastic Platform
          - Middleware for the execution of custom applications, their communication, and data storage is offered via a self-service interface over a network.
          - How do Cloud Offerings providing Execution Environments behave and how should they be used in applications?
          - One of the fundamental cloud properties is the sharing of resources among a large number of customers to leverage economy of scale. Extending resource sharing between customers to the operating systems and middleware increases the beneficial effects of economies of scale as the utilization of these resources can be increased.
          - Application components of different customers are hosted on shared middleware provided and maintained by the provider. Customers may deploy custom application components to this middleware using a self-service interface. This unification enables resource sharing and an automation of certain management tasks on the provider side, for example, provisioning of applications, update management.
        - Node-based Availability
          - A cloud provider guarantees the availability of individual nodes, such as individual virtual servers, middleware components or hosted application components.
          - How can providers express availability in a node-centric fashion, so that customers may estimate the availability of hosted applications?
          - A provider offers an Elastic Infrastructure or an Elastic Platform and needs a means to express the availability for the offerings from which the customer may then compute the availability of the hosted application. First, conditions are defined that have to be fulfilled by an available offering. Second, the timeframe needs to be expressed for which the provider assures this availability.
          - The provider assures availability for each hosted application component, which is defined to be available if it is reachable and performs its function as advertised, i.e., it provides correct results. This timeframe is often expressed as a percentage. An availability of 99.95%, thus, means that a hosted component will be available during 99.95% of the time it is hosted at the provider.
        - Environment-based Availability
          - A cloud provider guarantees the availability of the environment hosting individual nodes, such as virtual servers or hosted application components.
          - How can providers express availability in an environmental-centric fashion, so that customers may estimate the availability of hosted applications?
          - A cloud provider offers an Elastic Infrastructure or an Elastic Platform on which customers may deploy application components. The availability of this environment has to be expressed so that customers may match their requirements.
          - The provider assures availability for the provided environment, thus, for the availability of the Elastic Platform or the Elastic Infrastructure as a whole, for example, the availability for at-least-once provisioned component or virtual server and the availability to provision replacements in case of failures is assured. There is no notion of availability for individual application components or virtual servers deployed in this environment.
      - Processing Offerings
        - Patterns of this category describe how computation can be performed in the cloud.
        - Hypervisor
          - To enable the elasticity of clouds, the time required to provision and decommission servers is reduced through hardware virtualization.
          - How can virtual hardware that has been abstracted from physical hardware be used in applications?
          - If multiple applications are deployed on a physical server they may have to consider the other applications in their configuration. For example, if applications require the same network ports, access the same directories in the local file system etc. This sharing of common underlying physical hardware between different applications shall be simplified while also reducing dependencies of the application on the physical server.
          - A Hypervisor abstracts the hardware of a shared physical server into virtualized hardware. On this virtual hardware, different operating systems and middleware are installed to host applications sharing the physical server while being isolated from each other regarding the use of physical hardware, such as central processing units (CPU), memory, disk storage, and networking.
        - Execution Environment
          - To avoid duplicate implementation of functionality, application components are deployed to a hosting environment providing middleware services as well as often used functionality.
          - How can multiple application components share a hosting environment efficiently?
          - Applications often use similar functions, for example, to access networking interfaces, display user interfaces, access storage of the server etc. In this case, each application implements similar components that could be shared with other applications. Sharing such common functionality between applications would also result in a better utilization of the environment.
          - Common functionality is summarized in an Execution Environment providing functionality in platform libraries to be used in custom application implementations and in the form of the middleware. The environment, thus, executes custom application components and provides common functionality for data storages, communication etc.
        - Map Reduce
          - Large data sets to be processed are divided into smaller data chunks and distributed among processing application components. Individual results are later consolidated.
          - How can the performance of complex processing of large data sets be increased through scaling out?
          - Cloud applications often have to handle very large amounts of data, which have to be processed efficiently. As Distributed Applications are designed to scale out, data processing should be distributed among multiple application component instances in a similar means. Afterwards, results of these distributed components have to be consolidated.
          - A large data set to be processed is split up and mapped to multiple application components handling data processing. Data Processing Components simultaneously execute the query to be performed on the assigned data chunks. Afterwards, the individual results of all Processing Components are consolidated or reduced into one result data set. During this reduction, additional functions, such calculations of sums, average values etc. may be used.
      - Storage Offerings
        - Patterns of this category describe how data can be stored in the cloud.
        - Block Storage
          - Centralized storage is integrated into servers as a local hard drive managed by the operating system to enable access to this storage via the local file system.
          - How can central storage be accessed as a local drive by servers and hosted applications?
          - Virtual and non-virtualized servers offered as Infrastructure as a Service (IaaS) can be managed significantly easier if they do not store any state information locally, i.e., on their (virtual) hard drives. This eases their provisioning, decommissioning, and failure handling.
          - Centralized storage is accessed by servers as if it was a local hard drive, also referred to as block device.
        - Blob Storage
          - Data is provided in form of large files that are made available in a file system-like fashion by Storage Offerings that provides elasticity.
          - How can large files be stored, organized and made available over a network?
          - Distributed cloud applications often need to handle large data elements, also referred to as binary large objects (blob). Examples are virtual server images managed in an Elastic Infrastructure, pictures, or videos.
          - Data elements are organized in a folder hierarchy similar to a local file system. Each data element is given a unique identifier comprised of its location in the folder hierarchy and a file name. This unique identifier is passed to the storage offerings to retrieve a file over a network.
        - Relational Database
          - Data is structured according to a schema that is enforced during data manipulation and enables expressive queries of handled data.
          - How can data elements be stored so that relations between them can be expressed and expressive queries are enabled to retrieve required information effectively?
          - Handled data is often comprised of large numbers of similar data elements. These data elements have certain dependencies among each other. If such structured data is queried, clients make certain assumptions about the data structure and the consistency of relations between the retrieved data elements.
          - Data elements are stored in tables where each column represents an attribute of a data element. Table columns may have dependencies in the way that entries in one table column must also be present in a corresponding column of a different table. These dependencies are enforced during data manipulations.
        - Key-Value Storage
          - Semi-structured or unstructured data is stored with limited querying support but high-performance, availability, and flexibility.
          - How can key-value elements be stored to support scale out and an adjustable data structure?
          - To ensure availability and performance, a data storage offering shall be distributed among different IT resources and locations. Furthermore, changes of requirements or the fact that customers share a storage offering and have different requirements, raises the demand for a flexible data structure. as data structure validation during queries requires high-performance connectivity between distributed resources storing the data elements.
          - Pairs of identifiers (key) and associated data (value) are stored. No database schema or only a very limited one are supported to enforce a data structure. The expressiveness of queries is reduced significantly in favor of scalability and configurability: semi-structured on unstructured data can be scaled out among many IT resources without the need to access many of them for the evaluation of expressive queries.
        - Strict Consistency
          - Data is stored at different locations (replicas) to improve response time and to avoid data loss in case of failures while consistency of replicas is ensured at all times.
          - How can data be distributed among replicas to increase availability, while ensuring data consistency at all times?
          - To ensure failure tolerance, a storage offering duplicates data among multiple replicas. These replicas store the same set of data, so in case any of these replicas is lost, data may still be obtained and recovered from the other replicas.
          - Data is duplicated among several replicas to increase availability. A subset of data replicas is accessed by read and write operations. The ratio of the number of replicas accessed during read (r) and write (w) operations guarantees consistency: n < r + w
        - Eventual Consistency
          - If data is stored at different locations (replicas) to improve response time and avoid data loss in case of failures. Performance and the availability of data in case of network partitioning are enabled by ensuring data consistency eventually and not at all times.
          - How can data be distributed among replicas with focus on increased availability and performance, while being resilient towards connectivity problems?
          - Using multiple replicas of data is vital to ensure resiliency of a storage offering towards resource failures. Keeping all these replicas in a consistent state, however, requires a significant overhead as multiple or all data replicas have to be accessed during read and write operations.
          - The consistency of data is relaxed. This reduces the number of replicas that have to be accessed during read and write operations. Data alterations are eventually transferred to all replicas by propagating them asynchronously over the connection network.
      - Communication Offerings
      - Patterns of this category describe how data can be exchanged in the cloud.
        - Virtual Networking
          - Networking resources are virtualized to empower customers to configure networks, firewalls, and remote access using a self-service interface.
          - How can network connectivity between IT resources hosted in a cloud be configured dynamically and on-demand?
          - Application components deployed on Elastic Infrastructures and Elastic Platforms rely on physical network hardware to communicate with each other and the outside world. On this networking layer, different customers shall be isolated from each.
          - Physical networking resources, such as networking interface cards, switches, routers etc. are abstracted to virtualized ones. These Virtual Networking resources may share the same physical networking resources. Configuration is handled by customers through self-service interfaces.
        - Message-oriented Middleware
          - Asynchronous message-based communication is provided while hiding complexity resulting from addressing, routing, or data formats from communication partners to make interaction robust and flexible.
          - How can communication partners exchange information asynchronously with a communication partner?
          - The application components of a Distributed Application are hosted on multiple cloud resources and have to exchange information with each other. Often, the integration with other cloud applications and non-cloud applications is also required.
          - Communication partners exchange information asynchronously using messages. The message-oriented middleware handles the complexity of addressing, availability of communication partners and message format transformation.
        - Exactly-once Delivery
          - For many critical systems duplicate messages are inacceptable. The messaging system ensures that each message is delivered exactly once by filtering possible message duplicates automatically.
          - How can it be assured that a message is delivered only exactly once to a receiver?
          - Message duplicity is a very critical design issue for Distributed Applications and or application components that exchange messages via a Message-oriented Middleware.
          - Upon creation, each message is associated with a unique message identifier. This identifier is used to filter message duplicates during their traversal from sender to receiver.
        - At-least-once Delivery
          - In case of failures that lead to message loss or take too long to recover from, messages are retransmitted to assure they are delivered at least once.
          - How can communication partners or a Message-oriented Middleware ensure that messages are received successfully?
          - Sometimes, message duplicity can be coped with by the application using a Message-oriented Middleware. Therefore, for scenarios where message duplicates are uncritical, it shall still be ensured that messages are received.
          - For each message retrieved by a receiver an acknowledgement is sent back to the message sender. In case this acknowledgement is not received after a certain time frame, the message is resend.
        - Transaction-based Delivery
          - Clients retrieve messages under a transactional context to ensure that messages are received by a handling component.
          - How can it be ensured that messages are only deleted from a message queue if they have been received successfully?
          - While a Message-oriented Middleware can control traversing messages, it may be necessary to assure that messages are actually received successfully from a message queue by the client.
          - The Message-oriented Middleware and the client reading a message from a queue participate in a transaction. All operations involved in the reception of a message are, therefore, performed under one transactional context guaranteeing ACID behavior.
        - Timeout-based Delivery
          - Clients acknowledge message receptions to ensure that messages are received properly.
          - How can it be ensured that messages are only deleted from a message queue if they have been received successfully at least once?
          - In addition to ensuring that messages are not lost while they are traversing a Message-oriented Middleware it may, thus, also be required to assure that they are actually received by a client before they are deleted from a message queue.
          - To assure that a message is properly received, it is not deleted immediately after it has been read by a client, but is only marked as being invisible. In this state a message may not be read by another client. After a client has successfully read a message, it sends an acknowledgement to the message queue upon which reception the message is deleted.
    - Cloud Application Architectures
      - Patterns of this category describe how applications using cloud offerings can be built.
      - Fundamental Cloud Architectures
        - Patterns of this category cover the fundamental architectural styles that architects and developers have to be aware of when building a cloud-native application.
        - Loose Coupling
          - A communication intermediary separates application functionality from concerns of communication partners regarding their location, implementation platform, the time of communication, and the used data format.
          - How can dependencies between Distributed Applications and between individual components of these applications be reduced?
          - Information exchange between applications and their individual components as well as associated management tasks, such as scaling, failure handling, or update management can be simplified significantly if application components can be treated individually and the dependencies among them are kept to a minimum.
          - Communicating components and multiple integrated applications are decoupled from each other by interacting through a broker. This broker encapsulates the assumptions that communication partners would otherwise have to make about one other and, thus, ensures separation of concerns.
        - Distributed Application
          - A cloud application divides provided functionality among multiple application components that can be scaled out independently.
          - How can application functionality be decomposed to be handled by separate application components?
          - Applications have to respect the distribution and the scaling-out support of cloud environments in their architecture to efficiently benefit from it. Cloud applications, therefore, should to rely on multiple, possibly redundant IT resources. This can especially be the case if the cloud provider assures Environment-based Availability - the availability of the complete environment and not of single IT resources hosted in it.
          - The functionality of the application is divided into multiple independent components that provide a certain function. This componentization of application functionality introduces a logical decomposition of the application. These logical components are subsumed to multiple tiers to denote that they shall be deployed together physically, i.e., on one server (cluster).
          - Layer-based decomposition divides the application into separate logical layers. Components are restricted to access components of the same layer or one layer below.
          - Process-based decomposition focuses on the business processes supported by the application. These processes are comprised out of activities that are executed in a specific order. Functionality is divided into components with respect to the supported business activity.
          - Pipes-and-filters-based decomposition focues on for data-centric processing of an application. Each filter provides a certain function that is performed on input data and produces output data after processing. Multiple filters are interconnected with pipes, i.e, through messaging.
      - Cloud Application Components
        - Patterns of this category refine how functionality of a cloud application can be implemented in seperate components.
        - Stateful Component
          - Multiple instances of a scaled-out application component synchronize their internal state to provide a unified behavior.
          - How can applications components that are scaled-out maintain a synchronized internal state?
          - To benefit from a distributed cloud runtime environment, components of a Distributed Application are deployed to multiple cloud resources and their instances are scaled-out. Some of these application components may need to maintain an internal state, thus, the challenge arises that individual instances of application components should contain the same internal state, so that they present a unified behavior.
          - The internal state maintained by application component instances is replicated among all instances. Only small portions of shared information are used, for example, a configuration file stored centrally or configurations send by clients with every request.
        - Stateless Component
          - State is handled external of application components to ease their scaling-out and to make the application more tolerant to component failures.
          -  can elasticity and robustness of an application component be increased?
          - The components of a Distributed Application are deployed among multiple cloud resources to benefit from this distributed runtime environment through scaling out.The most significant factor complicating addition and removal of component instances in this scope is the internal state maintained by them. In case of failure, this information may even be lost.
          - Application components are implemented in a fashion that they do not have an internal state. Instead, their state and configuration is stored externally in Storage Offerings or provided to the component with each request.
        - User Interface Component
          - Interactive synchronous access to applications is provided to humans, while application-internal interaction is realized asynchronously when possible to ensure Loose Coupling. Furthermore, the user interface should be customizable to be used by different customers.
          - How can User Interface Components be accessed interactively by humans while being configurable and decoupled from the remaining application?
          - User Interface Component instances part of a Distributed Application have to be added and removed easily from the application without affecting the user experience. The dependencies on other application components should, therefore, be reduced as much as possible.
          - The User Interface Component serves as a bridge between the synchronous access of the human user and the asynchronous communication used with other application components. State information held externally, as described by the Stateless Component pattern. It is, therefore, attached to requests, may be held in a part of the user interface that is deployed on the user’s access device, or may be obtained from external storage. Instances of User Interface Components are scaled based on the number of synchronous requests to is as described by the Elastic Load Balancer pattern.
        - Processing Component
          - Possibly long running processing functionality is handled by separate components to enable elastic scaling. Processing functionality is further made configurable to support different customer requirements.
          - How can processing be scaled out elastically among distributed resources while being configurable regarding the supported functions to meet different customers’ requirements?
          - The processing functionality offered by an application shall be handled by different application component instances that operate independently. Instances of these components have to be added and removed easily to the application as part of scaling operations.
          - Processing functionality is split into separate function blocks and assigned to independent Processing Components. Each processing component is scaled out independently and is implemented in a stateless fashion as described in the Stateless Component pattern. Scaling is handled by an Elastic Queue. Data required for processing is provided with requests or by Storage Offerings.
        - Batch Processing Component
          - Requests are delayed until environmental conditions make their processing feasible.
          - How can asynchronous processing requests be delayed to be handled when conditions for their processing are optimal?
          - Distributed Applications assing processing functionality to different components to them independently. If such Processing Components are accessed asynchronously, certain conditions may make it unfeasible to process the requests immediately: seldom accesses to processing functionality, powerful Processing Component instances accessed continuously, and environmental conditions, such as resource costs.
          - Asynchronous processing requests are accepted at all times, but stored them until conditions are optimal for their processing. Based on the number of stored requests, environmental conditions, and custom rules, components are instantiated to handle the requests. Requests are only processed under non-optimal conditions if they cannot be delayed any longer.
        - Data Access Component
          - Functionality to store and access data elements is provided by special components that isolate complexity of data access, enable additional data consistency, and ensure adjustability of handled data elements to meet different customer requirements.
          - How can the complexity of data storage due to access protocols and data consistency be hidden and isolated while ensuring data structure configurability?
          - Handling the complexity of accessing data, i.e., handling of authorization, querying for data, failure handling etc. tightly couples application components to the used storage offering and complicates the implementation of these components as a lot of the idiosyncrasies of data handling have to be respected by them. Instead, different data sources should be integrated to provide a unified data access to other application components. Also, data may be stored at different cloud providers that have to be integrated as well.
          - Access to different data sources is integrated by a Data Access Component. This component coordinates all data manipulation. In case a storage offering shall be replaced or the interface of a storage offering changes, the Data Access Component is the only component that has to be adjusted.
        - Data Abstractor
          - The data provided to users or other application components is abstracted to inherently support eventually consistent data storage through the use of abstractions and approximations.
          - How can eventually consistent data be presented, so that possible inconsistencies are hidden from other application components and application users?
          - If a Distributed Application using eventually consistent Storage Offerings is designed for consistent data, data consistency could be reassured by application components, such as the Data Access Component. However, this can void the benefits introduced by eventually consistency regarding performance and availability.
          - The style of data representation is adjusted to allow retrieved data to be eventually consistent. The data representation always reflects that the consistent state is unknown by approximating values or abstracting them into more general ones, such as progress bars, traffic lights, or change tendencies (increase / decrease).
        - Idempotent Processor
          - Application functions detect duplicate messages and inconsistent data or are designed to be immune to these conditions.
          - How can an application component cope with message duplicates or data inconsistencies that could lead to duplicate function execution?
          - In case of Storage Offerings displaying Eventual Consistency, application components can possibly read obsolete information that has already been processed. Still, the component may choose to process the data again as changes cannot be seen. The same problem arises, if application components of a Distributed Application exchange information asynchronously via a Message-oriented Middleware assuring At-least-once Delivery. In this case duplicate messages can lead to duplicate processing.
          - The Idempotent Processor ensures that duplicate messages and inconsistent data do not affect application functionality either through inconsistency detection identifying message duplicates and data inconsistencies or through idempotent semantics of application functions enabling them to be erroneously executed multiple times with the same outcome.
        - Transaction-based Processor
          - Components receive messages or read data and process the obtained information under a transactional context to ensure that all received messages are processes and all altered data is consistent after processing, respectively.
          - How can an application component ensure that all messages it receives are processed successfully and altered data is persisted successfully after processing?
          - A Message-oriented Middleware can use Transaction-based Delivery of messages to ensure that messages are received successfully by a client. However, using this approach no assurances can be made regarding the processing of that received message.
          - Transaction-based Delivery subsumes reading the message from a queue and deleting it from a queue in one transaction. The Transaction-based Processor extends the transactional context to the processing of the message in the receiver. Analogous, if interacting with a storage offering, the Transaction-based Processor reads, processes and writes data in one transactional context.
        - Timeout-based Message Processor
          - Clients acknowledge message receptions and processing to ensure that all messages are handled by an application. If a message is not acknowledged after a certain timeout, it is processed by a different client.
          - How can an application process messages while guaranteeing that all messages handled by the application are processed at-least-once?
          - A Message-oriented Middleware uses Timeout-based Delivery to ensure that messages are received successfully by at least one client. Additionally, it shall be assured by the application that a message has also been properly processed after its reception.
          - Instead of sending an acknowledgement right after receiving a message, a timeout-based message processor sends this acknowledgement after it has successfully processed the message.
        - Multi-Component Image
          - Virtual servers host multiple application components that may not be active at all times to reduce provisioning and decommissioning operations.
          - How can a virtual server provide the functionality of multiple application components to be used flexibly in applications?
          - A Distributed Application may deploy its application components among virtual servers provided by an Elastic Infrastructure. The individual application components may, however, not fully utilize the servers if only one component is hosted per server. Therefore, mapping each application component to a single server may lead to underutilization.
          - Multiple application components (possibly including middleware) are hosted on a single virtual server to ensure that running virtual servers may be used for different purposes without making provisioning or decommissioning operations necessary.
      - Multi-Tenancy
        - Patterns of this category describe how cloud application can be shared between different customers if the application itself is offered as a service.
        - Shared Component
          - A component is accessed by multiple tenants to leverage economies of scale.
          - How can an application component be shared between multiple tenants enabling some individual configuration?
          - A Distributed Application is offered to multiple tenants. These tenants share IT resources required by applications provided to them. The provisioning of application component instances shall be optimized by limiting the portion of the application stack and the number of application components deployed exclusively for one tenant.
          - A Shared Component provides functionality that is equal for all tenants accessing the component. All tenants can be treated as a uniform user group to which a common user experience and service level is guaranteed.
        - Tenant-isolated Component
          - A component shared between tenants avoids influences between tenants regarding assured performance, available storage capacity, and accessibility of functionality and data.
          - How can an application component be shared between multiple tenants enabling individual configuration and tenant-isolation regarding performance, data volume, and access privileges?
          - A Distributed Application is offered to multiple tenants. These tenants share IT resources required by applications provided to them. However, the sharing of application components is hindered by three factors. First, tenants may have unique requirements and, thus, expect application components to be configurable to their individual needs. Second, tenants may not trust each other. Third, tenants expect an application to behave as if a single tenant was the only one accessing it.
          - Components on all layers of the application stack are specifically developed to be used by different tenants. Especially, they ensure isolation between tenants by controlling tenant access, processing performance used, and separation of stored data.
        - Dedicated Component
          - Components providing critical functionality shall be provided exclusively to tenants while still allowing other components to be shared between tenants.
          - How can application components that cannot be shared be integrated into a multi-tenant application?
          - A Distributed Application is offered to multiple tenants. These tenants share IT resources required by applications provided to them.
          - Dedicated application components are provided exclusively for each tenant using the application.
      - Cloud Integration
        - Patterns of this category describe how applications distributed among multiple hosting environments can be integrated to provide a holistic user experience. This is especially relevant in a Hybrid Cloud.
        - Restricted Data Access Component
          - Data provided to clients from different environments is adjusted based on access restrictions.
          - How can an application component alter provided data based on access restrictions imposed on different environments?
          - A Distributed Application may host application components at different providers to match the individual requirements of components with best fitting providers. One factor may be that application components experience different workload. Other differentiating factors of the used environments may be assured privacy, security, and trust. These differences may, however, impact the data that may be accessible in an environment.
          - Data storage restrictions and access privileges are defined for each data element. Access to these data elements is provided by separate Restricted Data Access Components that interpret the information associated with data elements. It adjusts data accordingly through deletion or obfuscation during every access.
        - Message Mover
          - Messages are moved automatically between different cloud providers to provide unified access to application components using messaging.
          - How can message queues of different providers be integrated without an impact on the application components using them?
          - The application components comprising a Distributed Application often exchange data using messages. If used message queues reside in different cloud environments that form a Hybrid Cloud accessibility to queues of one environment may be restricted for application components that are deployed in another environment.
          - A Message Mover is used to integrate message queues hosted in different environments by receiving messages from one queue and transferring it to a queue in other environments.
        - Application Component Proxy
          - An application component is made available in an environment from where it cannot be accessed directly by deploying an Application Component Proxy. The communication between this proxy and the application component is initiated and maintained from the environment where communication is unrestricted.
          - How can an application component be accessed if direct access to its hosting environment is restricted?
          - Application components of a Distributed Application are deployed in different Cloud Environments that form a Hybrid Cloud. These environments often have different privacy, security, and trust properties. The communication between environments is often restricted through the use of firewalls. However, application components hosted in unrestricted environments may have to access application components hosted in a restricted environment.
          - The interface of a restricted application component is duplicated to form a proxy component. Synchronous and asynchronous communication with this proxy component is initiated and maintained from the restricted environment that may access the unrestricted environment directly.
        - Compliant Data Replication
          - Data is replicated among multiple environments that may handle different data subsets. During replication data is obfuscated and deleted depending on laws and security regulations. Data updates are adjusted automatically to reflect the different data structures handled by environments.
          - How can data be replicated between environments if some environments may only handle subsets of the data due to laws and corporate regulations?
          - Distributed Applications that are hosted in a Hybrid Cloud often require access to the same data from different application components. If application components accessing the data are globally distributed, data access performance may be reduced drastically if data is only stored in one geographic location. Therefore, data may have to be replicated. Due to laws and corporate regulations, some of these locations may only handle a subset of the available data or data has to be obfuscated.
          - Data replicas in different environments are updated asynchronously using messaging. Message filters are used to delete and obfuscate certain data elements in these messages as they leave the trusted environment. Information about the data manipulations stored in a storage offering. If data is then altered in the less secure environment, the corresponding update message is enriched by a message enricher as it enters the secure environment.
        - Integration Provider
          - Integration functionality such as messaging and shared data is hosted by a separate provider to enable integrate of otherwise separated hosting environments.
          - How can application components that reside in different environments, possibly belonging to different companies, be integrated through a third-party provider?
          - When companies collaborate or one company has to integrate applications of different regional offices, different applications or the components of a Distributed Application are distributed among different hosting environments. Communication between these environments may be restricted and enabling communication may be hindered by corporate regulations.
          - The Distributed Applications or their components communicate using integration components offered by a third party provider.
    - Cloud Application Management
      - Patterns of this category describe how cloud applications can be managed automatically by separate components.
      - Management Components
        - Patterns of this category describe how management functionality can be integrated with components providing application functionality.
        - Provider Adapter
          - Provider interfaces are encapsulated and mapped to unified interfaces used in applications to separate concerns of interactions with the provider from application functionality.
          - How can the dependencies of an application component on a provider-specific interface be managed?
          - Cloud providers offer many interfaces that can be used in application components of a Distributed Application. If a component directly interacts with these interfaces, its implementation becomes strongly interleaved with the specific functions offered and the protocols used.
          - The Provider Adapter encapsulates all provider-specific implementations required for authentication, data formatting etc. in an abstract interface. The Provider Adapter, thus, ensures separation of concerns between application components accessing provider functionality and application components providing application functionality. It may also offer synchronous provider-interfaces to be accessed asynchronously via messages and vice versa.
        - Managed Configuration
          - Scaled-out application components should use a centrally stored configuration to provide a unified behavior that can be adjusted simultaneously.
          - How can the configuration of scaled out application component instances be controlled in a coordinated fashion?
          - Application components of a Distributed Application often have configuration parameters. Storing configuration information together with the application component implementation can be unpractical as it results in more overhead in case of configuration changes. Each running instance of the application component must be updated separately. Component images stored in Elastic Infrastructures or Elastic Platforms also have to be updated upon configuration change.
          - Configurations are stored in a central Storage Offering, commonly, a Relational Database, Key-Value Storage, or Blob Storage from where it is accessed by all running component instances either by accessing the storage periodically or by sending messages to the components.
        - Elasticity Manager
          - The utilization of IT resources on which an elastically scaled-out application is hosted, for example, virtual servers is used to determine the number of required application component instances.
          - How can the number of required application component instances be determined based on the utilization of hosting IT resources?
          - Application components of a Distributed Application hosted on an Elastic Infrastructure or Elastic Platform shall be scaled-out. The instances of applications components, thus, shall be provisioned and decommissioned automatically based on the current workload experienced by the application.
          - The utilization of cloud resources on which application component instances are deployed is monitored. This could be, for example, the CPU load of a virtual server. This information is used to determine the number of required instances.
        - Elastic Load Balancer
          - The number of synchronous accesses to an elastically scaled-out application is used to determine the number of required application component instances.
          - How can the number of required application component instances be determined based on monitored synchronous accesses?
          - Application components of a Distributed Application shall be scaled out automatically. Requests sent to an application shall be used as an indicator for the currently experienced workload from which the required number of components instances shall be deducted.
          - Based on the number of synchronous requests handled by a load balancer and possibly other utilization information, the required number of required component instances is determined.
        - Elastic Queue
          - The number of asynchronous accesses via messaging to an elastically scaled-out application is used to adjust the number of required application component instances.
          - How can the number of required application component instances be adjusted based on monitored asynchronous accesses?
          - A Distributed Application is comprised of multiple application components that are accessed asynchronously and deployed to an Elastic Infrastructure or an Elastic Platform. The required provisioning and decommissioning operations to scale this application should be performed in an automated fashion.
          - Queues that are used to distribute asynchronous requests among multiple application components instances are monitored. Based on the number of enqueued messages the Elastic Queue adjusts the number of application component instances handling these requests.
        - Watchdog
          - Applications cope with failures automatically by monitoring and replacing application component instances if the provider-assured availability is insufficient.
          - How can applications automatically detect failing application components and handle their replacement?
          - If a Distributed Application is comprised of many application components it is dependent on the availability of all component instances. To enable high availability under such conditions, applications have to rely on redundant application component instances and the failure of these instances has to be detected and coped with automatically.
          - Individual application components rely on external state information by implementing the Stateless Component pattern. Components are scaled out and multiple instances of them are deployed to redundant resources. The component instances are monitored by a separate Watchdog component and replaced in case of failures.
      - Management Processes
        - Patterns of this category describe how distributed and componentized cloud applications may address runtime challenges, such as elasticity and failure handling in an automated fashion.
        - Elasticity Management Process
          - Application component instances are added automatically to an application to cope with increasing workload. If the workload decreases application component instances are removed respectively.
          - How can the number of resources to which application components are scaled-out be adjusted efficiently to the currently experienced workload and anticipated future workload?
          - A Distributed Application uses Elasticity Managers, Elastic Queues, or Elastic Load Balancers to ensure an elastic scaling of application components. To handle this task adequately, the current resource demand has to be obtained automatically from the application and has to be reflected in provisioning and decommissioning of cloud resources.
          - An Elasticity Management Process analyzes the utilization of application component instances in intervals, when a system manager requests it, or if certain conditions are observed by the monitoring component. Based on this information, the current workload of the application is computed and reflected by adjusting used resources.
        - Feature Flag Management Process
          - If the cloud cannot provide required resources in time, the features provided by application components are degraded gracefully to replace or disable unimportant ones in order to keep vital features operational.
          - How can the performance of an application degrade gracefully, if the experienced workload increases but additional cloud resources are unavailable or take too long to provision?
          - While the elasticity of clouds generally allows a tight alignment of resource numbers to the experienced workload, the time it takes to provision new resources remains as a limiting factor. If the workload increases too drastically, it may take too long to provision new resources. Furthermore, cloud providers often do not guarantee concrete provisioning times.
          - Less important application functionality provided by application component instances is disabled or replaced with a less demanding implementation, if the cloud provider cannot fulfill current workload demands. When resources can eventually be provisioned again, the application components return to normal operation.
        - Update Transition Process
          - When a new application component version, middleware versions etc. become available, running application components are updated seamlessly.
          - How can application components of a Distributed Application be updated seamlessly?
          - During the runtime of a Distributed Application, new versions of used middleware, operating systems, or application components may become available. A seamless switch from the old to the new version of application components shall be enabled.
          - The new component version is created. Additional application component instances of the new version are provisioned. These components are executed simultaneously with the application components of the old version. If necessary, load balancing is then switched to the component instances of the new version. If the application components access a queue, this step is unnecessary. Finally, the old application component instances are decommissioned.
        - Standby Pooling Process
          - Application component instances should be kept on standby to increase provisioning speed and utilize billing time-slots efficiently.
          - How can defined provisioning times for application component instances be ensured while utilizing pay-per-use resources in an optimal fashion?
          - Even though application component instances may be provisioned and decommissioned dynamically, it usually requires some time to actually provision and decommission them. If a cloud application, however, experiences drastic and quick workload changes, these provisioning times may limit its capability to obtain the required resources quickly enough. Decommissioning of component instances immediately when no longer needed may also be ineffective, if cloud resources are charged for fixed time-slots.
          - Instead of decommissioning application component instances instantly when they are unused, they are assigned to a standby list They are decommissioned only when the time-slot they have been paid for has been utilized and they are still not needed. The standby list may always contain a certain number of component instances to ensure timely provisioning.
        - Resiliency Management Process
          - Application components are checked for failures and replaced automatically without human intervention.
          - How can the overall availability of an application be ensured automatically even if individual application component instances fail?
          - A Watchdog allows to monitor application components and react to failures. To handle this task, the component functionality must be verified and failing components must be replaced with newly provisioned components in a coordinated fashion.
          - This process is triggered by the monitoring functionality or by the Watchdog if it detects a component failure. Additionally, the Resiliency Management Process periodically verifies application component health. If a failure is detected, the faulty application component instance is decommissioned and replaced by a newly provisioned instance.
    - Composite Cloud Applications
      - Patterns of this category describe applications composed out of multiple cloud computing patterns.
      - Native Cloud Applications
        - Patterns of this category describe fundamental composite cloud applications.
        - Two-Tier Cloud Application
          - Presentation and business logic is bundled to one stateless tier that is easy to scale. This tier is separated from the data tier that is harder to scale and often handled by a provider-supplied storage offering.
          - How can application functionality be separated from data handling to scale them independently?
          - A Distributed Application is decomposed into application components to scale individual application functions independently. In this scope, data handling functionality is significantly harder to scale than Stateless Components, because Stateful Components have to coordinate state information between instances. Therefore, the application shall be decomposed in a fashion that separates the easy-to-scale functionality from the hard-to-scale functionality.
          - Application functionality is decomposed into data handling functionality, provided by one or several Storage Offerings, and application components handling presentation and business logic. This separation enables the two tiers to elastically scale independently with their workloads.
        - Three-Tier Cloud Application
          - The presentation, business logic, and data handling is realized as separate tiers to scale stateless presentation and compute-intensive processing independently of the data tier, which is harder to scale and often handled by the cloud provider.
          - How can presentation logic, business logic, and data handling be decomposed into separate tiers that are scaled independently?
          - A Distributed Application is decomposed into application components to scale individual application functions independently. There can be many differentiating factors of application tiers. For example, if Processing Components are more computation intensive or are used less frequently than User Interface Components, aligning the elastic scaling of these two components by summarizing their implementation in one tier can be inefficient. This issue arises every time components experiences different Application Workloads. The number of provisioned component instances cannot be aligned well to the different workloads if they are summarized to coarse grained tiers.
          - The application is decomposed into three tiers, where each tier is elastically scaled independently. The presentation tier is comprised of a load balancer and an application component that implements the Stateless Component pattern and User Interface Component pattern. The business logic tier is comprised of an application component implementing the Stateless Component pattern in addition to the Processing Component pattern.
        - Content Distribution Network
          - Applications component instances and data handled by them are globally distributed to meet the access performance required by a global user group.
          - How can timely access to an application be ensured for a globally distributed user group?
          - If the application provides multimedia content to users, for example, streamed videos and music the amount of data to be served increases drastically. If such multimedia content is located too far from the user accessing it, the communication delay of the distribution network may hinder the timely access to data. Therefore, storing content in only one centralized location, i.e., one cloud or data center is unfeasible.
          - Content replicas are established in different physical locations of one or multiple clouds. During this distribution of replicas, the topology of distribution networks is considered to ensure locality for all users. Replicas are updated from a central location.
      - Hybrid Cloud Applications
        - Patterns of this category describe applications providing concrete functionality as well as the distribution of application components in a Hybrid Cloud setup.
        - Hybrid User Interface
          - Varying workload from a user group interacting asynchronously with an application is handled in an elastic environment while the remainder of an application resides in a static environment.
          - How can a user interface for asynchronous interaction be hosted in a cloud while being integrated with an application otherwise hosted in a static data center?
          - An application serves user groups with different workload behavior. One user group generates Static Workload, while another user group generates Periodic Workload, Once-in-a-lifetime Workload, Unpredictable Workload, or Continuously Changing Workload. Since the predictability of the user group size and workload behavior differs, it shall be ensured that unexpected peak workloads do not affect the performance of the application while each user group is handled by the most suitable cloud environment.
          - The User Interface Component serving users generating varying workload is hosted in an elastic cloud environment. Other application components that are hosted in a static environment. The user interface deployed in the elastic cloud is integrated with the remainder of the application in a decoupled fashion using messaging to ensure Loose Coupling.
        - Hybrid Processing
          - Processing functionality that experiences varying workload is hosted in an elastic cloud while the remainder of an application resides in a static environment.
          - How can Processing Components that experiences varying workload be hosted in an elastic cloud while the remainder of an application is hosted in a static data center?
          - A Distributed Application provides processing functionality that experience different workload behavior. The user group accessing the application is, thus, predictable in size, but accesses the functions provided by the application differently. While most of the functions are used equally over time and, therefore, experience Static Workload, some Processing Components experience Periodic Workload, Unpredictable Workload, or Continuously Changing Workload.
          - The Processing Components experiencing varying workloads are provisioned in an elastic cloud. Loose Coupling is ensured by exchanging information between the hosting environments asynchronously via messages.
        - Hybrid Data
          - Data of varying size is hosted in an elastic cloud while the remainder of an application resides in a static environment.
          - How can a data handling functionality that experiences varying workload be hosted in an elastic cloud while the rest of the application is located in a static data center?
          - A Distributed Application handles data whose size varies drastically over time. Large amounts of data may, thus, be generated periodically and are then deleted again, may increase and decrease randomly, or may display a general increase or decrease over time. Especially, during these changes, the user number and their accesses to the application can be static resulting in Static Workload on the remainder of the application components.
          - Data whose varying size makes it unsuitable for hosting in a static environment is handled by Storage Offerings in an elastic cloud. At this location data is either accessed by Data Access Components that are hosted in the static environment or by Data Access Components hosted in the elastic environment.
        - Hybrid Backup
          - Data is periodically extracted from an application to be archived in an elastic cloud for disaster recovery purposes.
          - How can data be archived in a remote environment while the remainder of the application is hosted in a static environment?
          - Many applications are used by small and medium businesses which do not have the required IT skills to host and maintain their own highly available infrastructure. Especially, requirements regarding business resiliency - the ability to recover from an error - and business continuity - the ability to operate during an error - are challenging. Furthermore, there are laws and regulations making businesses liable to archive data and keep it accessible for audits, often over very long periods of time.
          - A Distributed Application is hosted in a local static environment of the company. Data handled by Stateful Components is periodically extracted and replicated to a cloud storage offering.
        - Hybrid Backend
          - Backend functionality comprised of data intensive processing and data storage is experiencing varying workloads and is hosted in an elastic cloud while the rest of an application is hosted in a static data center.
          - How can Processing Components that experience varying workload and need access to large amounts of data be hosted in an elastic environment while the remainder of the application is hosted in a static environment?
          - A Distributed Application provides processing functionality that experiences varying workload behavior. Mainly, Static Workload has to be handled, but some Processing Components experience Periodic Workload, Unpredictable Workload, or Continuously Changing Workload. Application components providing the respective processing functionality experiencing varying workload should, therefore, be hosted in an elastic environment. However, these components have to access large amounts of data during their execution making them highly dependent on the availability and the timely access to such data.
          - The Processing Components experiencing varying workloads are hosted in an elastic cloud together with the data accessed during their operation. Processing Components in the elastic cloud are triggered from the static environment through asynchronous messages exchanged via message queues provided by a Message-oriented Middleware. A Data Access Component in the static environment ensures that data required by elastic Processing Components is stored in Storage Offerings The location where this data is stored may then be passed to the elastic Processing Components during their enactment via messages. Data that is not required by the backend functionality may still be stored in Stateful Components hosted in the static data center.
        - Hybrid Application Functions
          - Some application functionality provided by user interfaces, processing, and data handling is experiencing varying workload and is hosted in an elastic cloud while other application functionality of the same type is hosted in a static environment.
          - How can arbitrary functionality of an application be distributed among static data centers and elastic clouds best matching its requirements?
          - Application components comprising a Distributed Application experience varying workloads on all layers of the application stack: User Interface Components, Processing Components, and Data Access Components. All of these components provide functionality to the user group of the application, but this user group accesses functionality differently. In addition to the workload requirements other issues, such as legal and corporate regulations or requirements on security, privacy, and trust may limit the environments to which an application component may be provisioned.
          - Application components are grouped regarding similar requirements and are deployed into best fitting environments. Interdependencies between the components are reduced by exchanging data using asynchronous messaging to ensure Loose Coupling. Depending on the accessed function, a load balancer redirects user accesses to the different environments seamlessly.
        - Hybrid Multimedia Web Application
          - Website content is largely provided from a static environment. Multimedia files that cannot be cached efficiently are provided from a large distributed elastic environment for high-performance access.
          - How can non-cacheable content be integrated efficiently in a website that is accessed by a large globally distributed user group?
          - A Distributed Application provides a website accessed by a large globally distributed user group. While most of the website is comprised of static content, there is also a significant amount of multimedia content, such as videos or music that has to be streamed to users.
          - Static website content is hosted in a static environment from where it is accessed by users. The streaming content is provided by an elastic cloud environment where it is accessed from the application’s User Interface Component. The static content is provided to users’ client software and in this static content, the multimedia content is referenced. Retrieval of this streaming content is often handled directly by the users’ browser software.
        - Hybrid Development Environment
          - A production runtime environment is replicated and mocked in an elastic environment where new applications can be developed and tested.
          - How can an application use different computing environments during its development, test, and production stages?
          - Applications have different requirements on the runtime environment during their development, test, and production phase. During development, hardware requirements are often uncertain, so hardware resources should be flexible to extend resources if necessary. During the test phase, diverse test systems may be needed to verify the proper functioning of the application on different operating systems or when being accessed using different client software, i.e., different browsers. Large numbers of resources may also be required to perform load tests. During the productive use other factors, such as security and availability may be of greater importance than resource flexibility.
          - The production environment of the application is simulated in the development and test environment through the use of equivalent addressing, similar mocking data, and equivalent functionality provided by the environment. Migration of developed applications is ensured through transformation of application components or compatibility of runtimes. Some testing resources are provided exclusively in the development environment to verify the application behavior under different circumstances.
  - Cloud Architecture Qualities
    - Availability
      - Availability is the proportion of time that the system is functional and working, usually measured as a percentage of uptime. It can be affected by system errors, infrastructure problems, malicious attacks, and system load. Cloud applications typically provide users with a service level agreement (SLA), so applications must be designed to maximize availability.
    - Data Management
      - Data management is the key element of cloud applications, and influences most of the quality attributes. Data is typically hosted in different locations and across multiple servers for reasons such as performance, scalability or availability, and this can present a range of challenges. For example, data consistency must be maintained, and data will typically need to be synchronized across different locations.
    - Design and Implementation
      - Good design encompasses factors such as consistency and coherence in component design and deployment, maintainability to simplify administration and development, and reusability to allow components and subsystems to be used in other applications and in other scenarios. Decisions made during the design and implementation phase have a huge impact on the quality and the total cost of ownership of cloud hosted applications and services.
    - Messaging
      - The distributed nature of cloud applications requires a messaging infrastructure that connects the components and services, ideally in a loosely coupled manner in order to maximize scalability. Asynchronous messaging is widely used, and provides many benefits, but also brings challenges such as the ordering of messages, poison message management, idempotency, and more
    - Management and Monitoring
      - Cloud applications run in in a remote datacenter where you do not have full control of the infrastructure or, in some cases, the operating system. This can make management and monitoring more difficult than an on-premises deployment. Applications must expose runtime information that administrators and operators can use to manage and monitor the system, as well as supporting changing business requirements and customization without requiring the application to be stopped or redeployed.
    - Performance and Scalability
      - Performance is an indication of the responsiveness of a system to execute any action within a given time interval, while scalability is ability of a system either to handle increases in load without impact on performance or for the available resources to be readily increased. Cloud applications typically encounter variable workloads and peaks in activity. Predicting these, especially in a multi-tenant scenario, is almost impossible. Instead, applications should be able to scale out within limits to meet peaks in demand, and scale in when demand decreases. Scalability concerns not just compute instances, but other elements such as data storage, messaging infrastructure, and more.
    - Resiliency
      - Resiliency is the ability of a system to gracefully handle and recover from failures. The nature of cloud hosting, where applications are often multi-tenant, use shared platform services, compete for resources and bandwidth, communicate over the Internet, and run on commodity hardware means there is an increased likelihood that both transient and more permanent faults will arise. Detecting failures, and recovering quickly and efficiently, is necessary to maintain resiliency.
    - Security
      - Security is the capability of a system to prevent malicious or accidental actions outside of the designed usage, and to prevent disclosure or loss of information. Cloud applications are exposed on the Internet outside trusted on-premises boundaries, are often open to the public, and may serve untrusted users. Applications must be designed and deployed in a way that protects them from malicious attacks, restricts access to only approved users, and protects sensitive data.
  - Cloud Architecture Patterns
    - Ambassador
      - Create helper services that send network requests on behalf of a consumer service or application.
    - Anti-Corruption Layer
      - Implement a façade or adapter layer between a modern application and a legacy system.
    - Backends for Frontends
      - Create separate backend services to be consumed by specific frontend applications or interfaces.
    - Bulkhead
      - Isolate elements of an application into pools so that if one fails, the others will continue to function.
    - Cache-Aside
      - Load data on demand into a cache from a data store
    - Circuit Breaker
      - Handle faults that might take a variable amount of time to fix when connecting to a remote service or resource.
    - Claim Check
      - Split a large message into a claim check and a payload to avoid overwhelming a message bus.
    - Compensating Transaction
      - Undo the work performed by a series of steps, which together define an eventually consistent operation.
    - Competing Consumers
      - Enable multiple concurrent consumers to process messages received on the same messaging channel.
    - Compute Resource Consolidation
      - Consolidate multiple tasks or operations into a single computational unit
    - CQRS
      - Segregate operations that read data from operations that update data by using separate interfaces.
    - Event Sourcing
       - Use an append-only store to record the full series of events that describe actions taken on data in a domain.
    - External Configuration Store
      - Move configuration information out of the application deployment package to a centralized location.
    - Federated Identity
      - Delegate authentication to an external identity provider.
    - Gatekeeper
      - Protect applications and services by using a dedicated host instance that acts as a broker between clients and the application or service, validates and sanitizes requests, and passes requests and data between them.
    - Gateway Aggregation
      - Use a gateway to aggregate multiple individual requests into a single request.
    - Gateway Offloading
      - Offload shared or specialized service functionality to a gateway proxy.
    - Gateway Routing
      - Route requests to multiple services using a single endpoint.
    - Health Endpoint Monitoring
      - Implement functional checks in an application that external tools can access through exposed endpoints at regular intervals.
    - Index Table
      - Create indexes over the fields in data stores that are frequently referenced by queries.
    - Leader Election
      - Coordinate the actions performed by a collection of collaborating task instances in a distributed application by electing one instance as the leader that assumes responsibility for managing the other instances.
    - Materialized View
      - Generate prepopulated views over the data in one or more data stores when the data isn't ideally formatted for required query operations.
    - Pipes and Filters
      - Break down a task that performs complex processing into a series of separate elements that can be reused.
    - Priority Queue
      - Prioritize requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority.
    - Publisher/Subscriber
      - Enable an application to announce events to multiple interested consumers aynchronously, without coupling the senders to the receivers.
    - Queue-Based Load Leveling
      - Use a queue that acts as a buffer between a task and a service that it invokes in order to smooth intermittent heavy loads.
    - Retry
      - Enable an application to handle anticipated, temporary failures when it tries to connect to a service or network resource by transparently retrying an operation that's previously failed.
    - Scheduler Agent Supervisor
      - Coordinate a set of actions across a distributed set of services and other remote resources.
    - Sharding
      - Divide a data store into a set of horizontal partitions or shards.
    - Sidecar
      - Deploy components of an application into a separate process or container to provide isolation and encapsulation.
    - Static Content Hosting
      - Deploy static content to a cloud-based storage service that can deliver them directly to the client.
    - Strangler
      - Incrementally migrate a legacy system by gradually replacing specific pieces of functionality with new applications and services.
    - Throttling
      - Control the consumption of resources used by an instance of an application, an individual tenant, or an entire service.
    - Valet Key
      - Use a token or key that provides clients with restricted direct access to a specific resource or service.
  - Cloud Architecture Concepts
    - Service Mesh
      - Container orchestration framework.
        - As more and more containers are added to an application’s infrastructure, a separate tool for monitoring and managing the set of containers - a container orchestration framework - becomes essential.
      - Services and instances (Kubernetes pods).
        - An instance is a single running copy of a microservice.
        - Sometimes the instance is a single container; in Kubernetes, an instance is made up of a small group of interdependent containers (called a pod).
        - Clients rarely access an instance or pod directly; rather they access a service, which is a set of identical instances or pods (replicas) that is scalable and fault‑tolerant.
      - Sidecar proxy.
        - A sidecar proxy runs alongside a single instance or pod.
        - The purpose of the sidecar proxy is to route, or proxy, traffic to and from the container it runs alongside.
        - The sidecar communicates with other sidecar proxies and is managed by the orchestration framework.
        - Many service mesh implementations use a sidecar proxy to intercept and manage all ingress and egress traffic to the instance or pod.
      - Service discovery.
        - When an instance needs to interact with a different service, it needs to find - discover - a healthy, available instance of the other service.
        - Typically, the instance performs a DNS lookup for this purpose.
        - The container orchestration framework keeps a list of instances that are ready to receive requests and provides the interface for DNS queries.
      - Load balancing.
        - Most orchestration frameworks already provide Layer 4 (network) load balancing. 
        - A service mesh implements more sophisticated Layer 7 (application) load balancing, with richer algorithms and more powerful traffic management.
        - Load‑balancing parameters can be modified via API, making it possible to orchestrate blue‑green or canary deployments.
      - Encryption.
        - The service mesh can encrypt and decrypt requests and responses, removing that burden from each of the services.
        - The service mesh can also improve performance by prioritizing the reuse of existing, persistent connections, which reduces the need for the computationally expensive creation of new ones.
        - The most common implementation for encrypting traffic is mutual TLS (mTLS), where a public key infrastructure (PKI) generates and distributes certificates and keys for use by the sidecar proxies.
      - Authentication and authorization.
        - The service mesh can authorize and authenticate requests made from both outside and within the app, sending only validated requests to instances.
      - Support for the circuit breaker pattern.
        - The service mesh can support the circuit breaker pattern, which isolates unhealthy instances, then gradually brings them back into the healthy instance pool if warranted.
    - API Gateway
  - Solutions features
    - Skaffold
      - Remote development
        - Skaffold doesn’t require you to run a local Kubernetes cluster (minikube or docker-for-desktop).
        - It can build/push images locally with docker, and run them on the remote clusters (such as GKE).
      - More remote development
        - You actually don’t need to run a local docker either. Skaffold can do remote builds using services like Google Container Builder. Although it’ll be slow.
      - Tag management
        - In your Kubernetes manifests, you leave the image tags out in the “image:” field, and Skaffold automatically changes the manifests with the new tags as it rebuilds the images.
      - Rebuild only what’s changed
        - If your microservices are on separate directories, changing source code for one will not cause rebuild for all images.
        - Skaffold understands which images have been impacted by the change.
      - Cleanup on exit
        - Terminating “skaffold dev” runs a routine that cleans up the deployed k8s resources.
        - If this fails, you can run “skaffold delete” to clean up deployed artifacts.
    - Bazel Build System
      - Capabilities
        - Incremental
          - Rebuild time is proportional to what you changed
        - Deterministic
          - We can cache build results based on their inputs
        - Hermetic
          - allows features like remote execution, parallelization
        - Composable
          - Bazel plugins are like Unix pipes, allows novel chaining
        - Universal
          - Builds Android, iOS, web, backends, cloud services, and more
        - Industrial grade
          - Googlers have beat on this thing in every way
    - Istio Service Mesh
      - Capabilities
        - traffic monitoring
        - access control
        - discovery
        - security
        - resiliency
        - canary deployments
        - fault injections
        - tracing
        - circuit breakers
        - routing
        - load balancing
        - retries
        - timeouts
        - mirroring
        - rate limiting
      - Components
        - Envoy
          - Envoy is an open source extension and service proxy provider, built for cloud-extensive meshes. The Istio mesh creates an extendible proxy system through Envoy.
        - Mixer
          - The mixer is a part of the service mesh that helps in enforcing safety protocols, allowing access controls and implementing usage policies and works independently from the mesh.
        - Pilot
          - Pilot provides all the services for the Istio Envoy sidecars and allows for a more coherent traffic management system with high-level routing.
      - Use Cases
        - Finding and Recognizing Services
          - It’s common for organizations to be unaware of which services are running in their infrastructure, which becomes worse for a microservices-based environment.
          - Istio service mesh provides service-level visibility and telemetry that helps any organization be updated with service inventories and dependency analysis.
        - Operation Reliability
          - The telemetry data service tells you how well a service is performing, such as the time taken to respond to service requests, which resources were used, and how often they were used.
          - This helps developers to spot issues and correct them before they cause any repercussions to the wider application environment.
        - Structured Traffic Governance
          - In the case that any organization thinks about sidelining or restricting specific content such as URLs or sub-URLs, the Istio service mesh allows for such arrangements for any range of traffic management systems.
          - With Istio, this can be done without having to refactor the application by simply using the sidecar functionalities of Istio.
          - This includes services within a specific mesh as well as the ingress and egress traffic that exits and enters the mesh.
        - Safer Service-to-Service Communications
          - As the Istio service mesh allows a secure universal service identity system, companies can use a mutually integrated TLS for service-to-service communications.
          - This also allows users to add service-level authentication procedures employing either TLS or JSON Web Tokens (JWS).
        - Systems for Trust-Based Access Control
          - Instead of configuring access to mainframe systems based on common static attributes, such as user identities, IP addresses, or access control lists, service meshes like Istio allow for real-time hosting as well as using network telemetry on the data.
          - For instance, users can draft and execute a safety policy that states that every service request can be accessed based on the purpose of the request or might even demand a Certificate Signing Request (CSR) that becomes a valid id should the requester pass a string of confirmatory checks.
        - Measures for Drastic Times
          - Service meshes are equipped with specific functions that perform fault injection procedures and test the resiliency of services.
          - Istio service mesh can inject specific delays in the service responses to see how the application executes and responds to requesters as a whole component.
          - Injecting delays is also a tried and true method of modern chaos engineering techniques that are used to raise the longevity and resilience of the systems against faulty situations.
    - Programming Language
      - JavaScript Engine
        - Event Table
        - Event Queue
        - Event Loop
        - Lexical Structure
        - Expressions
        - Types
        - Variables
        - Functions
        - this
        - Arrow Functions
        - Loops
        - Loops and Scope
        - Arrays
        - Template Literals
        - Semicolons
        - Strict Mode
        - ECMAScript 6, 2016, 2017
      - Golang
    Runtime Environments
    - Node.js
      - The V8 JavaScript Engine
        - V8 provides the runtime environment in which JavaScript executes.
        - The DOM, and the other Web Platform APIs are provided by the browser.
      - Asynchronous programming and callbacks
      - Timers
      - Promises
      - Async and Await
      - Closures
      - The Event Loop
    - Go
  - Definitions
    - AJAX
      - Asynchronous JavaScript and XML (known as AJAX) is a term that describes a new approach to using multiple technologies together in order to enable web applications to make quick updates to the user interface without reloading the entire browser page.
    - API
      - API stands for Application Programming Interface and is a set of features and rules provided by a provided by a software to enable third-party software to interact with it.
      - The code features of a web API usually include methods, properties, events or URLs.
    - Argument
      - An argument is a value passed as an input to a function and can be either a primitive or an object.
      - In JavaScript, functions can also be passed as arguments to other functions.
    - Array
      - Arrays are used to store multiple values in a single variable.
      - Arrays are ordered and each item in an array has a numeric index associated with it.
      - JavaScript arrays are zero-indexed, meaning the first element's index is 0.
    - Asynchronous programming
      - Asynchronous programming is a way to allow multiple events to trigger code without waiting for each other.
      - The main benefits of asynchronous programming are improved application performance and responsiveness.
    - Automatic semicolon insertion
      - Automatic semicolon insertion (ASI) is a JavaScript feature that allows developers to omit semicolons in their code.
    - Boolean
      - Booleans are one of the primitive data types in JavaScript.
      - They represent logical data values and can only be `true` or `false`.
    - Callback
      - A callback function, also known as a high-order function, is a function that is passed into another function as an argument, which is then executed inside the outer function.
      - Callbacks can be synchronous or asynchronous.
    - Character encoding
      - A character encoding defines a mapping between bytes and text, specifying how the sequenece of bytes should be interpreted.
      - Two commonly used character encodings are ASCII and UTF-8.
    - Class
      - In object-oriented programming, a class is a template definition of an object's properties and methods.
    - Closure
      - A closure is the combination of a function and the lexical environment within which that function was declared.
      - The closure allows a function to access the contents of that environment.
    - CoffeeScript
      - CoffeeScript is a programming language inspired by Ruby, Python and Haskell that transpiles to JavaScript.
    - Constant
      - A constant is a value, associated with an identifier.
      - The value of a constant can be accessed using the identifier and cannot be altered during execution.
    - Constructor
      - In class-based object-oriented programming, a constructor is a special type of function called to instantiate an object.
      - Constructors often accept arguments that are commonly used to set member properties.
    - Continuous Deployment
      - Continuous Deployment follows the testing that happens during Continuous Integration and pushes changes to a staging or production system.
      - Continuous Deployment ensures that a version of the codebase is accessible at all times.
    - Continuous Integration
      - Continuous Integration (CI) is the practice of testing each change done to a codebase automatically and as early as possible.
      - Two popular CI systems that integrate with GitHub are Travis CI and Circle CI.
    - CORS
      - Cross-Origin Resource Sharing (known as CORS) is a mechanism that uses extra HTTP headers to tell a browser to let a web application running at one domain have permission to access resources from a server at a different domain.
    - Cross-site scripting (XSS)
      - XSS refers to client-side code injection where the attacker injects malicious scripts into a legitimate website or web application.
      - This is often achieved when the application does not validate user input and freely injects dynamic HTML content.
    - CSS
      - CSS stands for Cascading Style Sheets and is a language used to style web pages.
      - CSS documents are plaintext documents structured with rules, which consist of element selectors and property-value pairs that apply the styles to the specified selectors.
    - CSV
      - CSV stands for Comma-Separated Values and is a storage format for tabular data.
      - CSV documents are plaintext documents where each line represents a table row, with table columns separated by commas or some other delimiter (e.g. semicolons).
      - The first line of a CSV document sometimes consists of the table column headings for the data to follow.
    - Currying
      - Currying is a way of constructing functions that allows partial application of a function's arguments.
      - Practically, this means that a function is broken down into a series of functions, each one accepting part of the arguments.
    - Deserialization
      - Deserialization is the process of converting a format that has been transferred over a network and/or used for storage to an object or data structure.
      - A common type of deserialization in JavaScript is the conversion of JSON string into an object.
    - DNS
      - A DNS (Domain Name System) translates domain names to the IP addresses needed to find a particular computer service on a network.
    - DOM
      - The DOM (Document Object Model) is a cross-platform API that treats HTML and XML documents as a tree structure consisting of nodes.
      - These nodes (such as elements and text nodes) are objects that can be programmatically manipulated and any visible changes made to them are reflected live in the document.
      - In a browser, this API is available to JavaScript where DOM nodes can be manipulated to change their styles, contents, placement in the document, or interacted with through event listeners.
    - Domain name registrar
      - A domain name registrar is a company that manages the reservation of internet domain names.
      - A domain name registrar must be approved by a general top-level domain (gTLD) registry or a country code top-level domain (ccTLD) registry.
    - Domain name
      - A domain name is a website's address on the Internet, used primarily in URLs to identify the server for each webpage.
      - A domain name consists of a hierarchical sequence of names, separated by dots and ending with an extension.
    - Element
      - A JavaScript representation of a DOM element commonly returned by `document.querySelector()` and `document.createElement()`.
      - They are used when creating content with JavaScript for display in the DOM that needs to be programatically generated.
    - ES6
      - ES6 stands for ECMAScript 6 (also known as ECMAScript 2015), a version of the ECMAScript specification that standardizes JavaScript.
      - ES6 adds a wide variety of new features to the specification, such as classes, promises, generators and arrow functions.
    - Event-driven programming
      - Event-driven programming is a programming paradigm in which the flow of the program is determined by events (e.g. user actions, thread messages, sensor outputs).
      - In event-driven applications, there is usually a main loop that listens for events and trigger callback functions accordingly when one of these events is detected.
    - Event loop
      - The event loop handles all asynchronous callbacks.
      - Callbacks are queued in a loop, while other code runs, and will run one by one when the response for each one has been received.
      - The event loop allows JavaScript to perform non-blocking I/O operations, despite the fact that JavaScript is single-threaded.
    - Express
      - Express is a backend framework, that provides a layer of fundamental web application features for Node.js.
      - Some of its key features are routing, middleware, template engines and error handling.
    - Factory functions
      - In JavaScript, a factory function is any function, which is not a class or constructor, that returns a new object.
      - Factory functions don't require the use of the `new` keyword.
    - First-class function
      - A programming language is said to have first-class functions if it treats them as first-class citizens, meaning they can be passed as arguments, be returned as values from other functions, be assigned to variables and stored in data structures.
    - Flexbox
      - Flexbox is a one-dimensional layout model used to style websites as a property that could advance space distribution between items and provide powerful alignment capabilities.
    - Function
      - Functions are self-contained blocks of code with their own scope, that can be called by other code and are usually associated with a unique identifier.
      - Functions accept input in the form of arguments and can optionally return an output (if no `return` statement is present, the default value of `undefined` will be returned instead).
      - JavaScript functions are also objects.
    - Functional programming
      - Functional programming is a paradigm in which programs are built in a declarative manner using pure functions that avoid shared state and mutable data.
      - Functions that always return the same value for the same input and don't produce side effects are the pillar of functional programming.
    - Functor
      - A Functor is a data type common in functional programming that implements a `map` method.
      - The `map` method takes a function and applies it to the data in the Functor, returning a new instance of the Functor with the result.
      - JavaScript `Array`s are an example of the Functor data type.
    - Garbage collection
      - Garbage collection is a form of automatic memory management.
      - It attempts to reclaim memory occupied by objects that are no longer used by the program.
    - Git
      - Git is an open-source version control system, used for source code management.
      - Git allows users to copy (clone) and edit code on their local machines, before merging it into the main code base (master repository).
    - Higher-order function
      - Higher-order functions are functions that either take other functions as arguments, return a function as a result, or both.
    - Hoisting
      - Hoisting is JavaScript's default behavior of adding declarations to memory during the compile phase.
      - Hoisting allows for JavaScript variables to be used before the line they were declared on.
    - HTML
      - HTML stands for HyperText Markup Language and is a language used to structure web pages.
      - HTML documents are plaintext documents structured with elements, which are surrounded by `<>` tags and optionally extended with attributes.
    - HTTP and HTTPS
      - The HyperText Transfer Protocol (HTTP) is the underlying network protocol that enables transfer of hypermedia documents on the Web, usually between a client and a server.
      - The HyperText Transfer Protocol Secure (HTTPS) is an encrypted version of the HTTP protocol, that uses SSL to encrypt all data transfered between a client and a server.
    - Integer
      - Integers are one of the primitive data types in Javascript.
      - They represent a numerical value that has no fractional component.
    - Integration testing
      - Integration testing is a type of software testing, used to test groups of units/components of a software.
      - The purpose of integration tests are to validate that the units/components interact with each other as expected.
    - IP
      - An IP address is a number assigned to a device connected to a network that uses the Internet protocol.
      - Two IP versions are currently in use - IPv4, the older version of the communication protocol (e.g. 192.168.1.100) and IPv6, the newest version of the communication protocol which allows for many different IP addresses (e.g. 0:0:0:0:ffff:c0a8:164).
    - jQuery
      - jQuery is a frontend JavaScript library, that simplifies DOM manipulation, AJAX calls and Event handling.
      - jQuery uses its globally defined function, `$()`, to select and manipulate DOM elements.
    - JSON
      - JSON (JavaScript Object Notation) is a format for storing and exchanging data.
      - It closely resembles the JavaScript object syntax, however some data types, such as dates and functions, cannot be natively represented and need to be serialized first.
    - MDN
      - MDN Web Docs, formerly known as Mozilla Developer Network, is the official Mozilla website for development documentation of web standards and Mozilla projects.
    - Module
      - Modules are independent, self-contained pieces of code that can be incorporated into other pieces of code.
      - Modules improve maintainability and reusability of the code.
    - MongoDB
      - MongoDB is a NoSQL database model that stores data in flexible, JSON-like documents, meaning fields can vary from document to document and data structure can be changed over time
    - Mutable value
      - Mutable value is a type of variable that can be changed once created.
      - Objects are mutable as their state can be modified after they are created.
      - Primitive values are not mutable as we perform reassignment once we change them.
    - MVC
      - MVC stands for Model-View-Controller and is a software design pattern, emphasizing separation of concerns (logic and display).
      - The Model part of the MVC pattern refers to the data and business logic, the View handles the layout and display, while the Controller routes commands to the model and view parts.
    - Node.js
      - Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine.
      - Node.js can execute JavaScript code outside of the browser and can be used to develop web backends or standalone applications.
    - NoSQL
      - NoSQL databases provide a mechanism to create, update, retrieve and calculate data that is stored in models that are non-tabular.
    - Npm
      - Npm is a package manager for the JavaScript programming language and the default package manager for Node.js.
      - It consists of a command-line client and the npm registry, an online database of packages.
    - Object-oriented programming
      - Object-oriented programming (OOP) is a programming paradigm based on the concept of objects, which may contain both data and procedures which can be use to operate on them.
      - JavaScript supports Object-oriented programming both via prototypes and classes.
    - Object
      - Objects are data structures that contain data and instructions for working with the data.
      - Objects consist of key-value pairs, where the keys are alphanumeric identifiers and the values can either be primitives or objects.
      - JavaScript functions are also objects.
    - Prepared statements
      - In databases management systems, prepared statements are templates that can be used to execute queries with the provided values substituting the template's parameters.
      - Prepared statements offer many benefits, such as reusability, maintainability and higher security.
    - Promise
      - The Promise object represents the eventual completion (or failure) of an asynchronous operation, and its resulting value.
      - A Promise can be in one of these states: pending(initial state, neither fulfilled nor rejected), fulfilled(operation completed successfully), rejected(operation failed).
    - Prototype-based programming
      - Prototype-based programming is a style of object-oriented programming, where inheritance is based on object delegation, reusing objects that serve as prototypes.
      - Prototype-based programming allows the creation of objects before defining their classes.
    - Pseudo-class
      - In CSS, a pseudo-class is used to define a special state of an element and can be used as a selector in combination with an id, element or class selector.
    - Pseudo-element
      - In CSS, a pseudo-element is used to style specific parts of an element and can be used as a selector in combination with an id, element or class selector.
    - PWA
      - Progressive Web App (known as PWA) is a term used to describe web applications that load like regular websites but can offer the user functionality such as working offline, push notifications, and device hardware access that were traditionally available only to native mobile applications.
    - React
      - React is a frontend framework, that allows developers to create dynamic, component-based user interfaces.
      - React separates view and state, utilizing a virtual DOM to update the user interface.
    - Recursion
      - Recursion is the repeated application of a process.
      - In JavaScript, recursion involves functions that call themselves repeatedly until they reach a base condition.
      - The base condition breaks out of the recursion loop because otherwise the function would call itself indefinitely.
      - Recursion is very useful when working with nested data, especially when the nesting depth is dynamically defined or unkown.
    - Regular expressions
      - Regular expressions (known as regex or regexp) are patterns used to match character combinations in strings.
      - JavaScript provides a regular expression implementation through the `RegExp` object.
    - Repository
      - In a version control system, a repository (or repo for short) is a data structure that stores metadata for a set of files (i.e. a project).
    - Responsive web design
      - Responsive web design is a web development concept aiming to provide optimal behavior and performance of websites on all web-enabled devices.
      - Responsive web design is usually coupled with a mobile-first approach.
    - Scope
      - Each function has its own scope, and any variable declared within that function is only accessible from that function and any nested functions.
    - Selector
      - A CSS selector is a pattern that is used to select and/or style one or more elements in a document, based on certain rules.
      - The order in which CSS selectors apply styles to elements is based on the rules of CSS specificity.
    - SEO
      - SEO stands for Search Engine Optimization and refers to the process of improving a website's search rankings and visibility.
    - Serialization
      - Serialization is the process of converting an object or data structure into a format suitable for transfer over a network and/or storage.
      - A common type of serialization in JavaScript is the conversion of an object into a JSON string.
    - Shadow DOM
      - Shadow DOM allows you to attach hidden DOM trees to elements in the normal DOM tree, which are included in the document rendering, but excluded from the main document DOM tree.
      - A shadow DOM tree will start with a shadow root, to which you can attach any elements you want, just like in a regular DOM.
      - Examples of shadow DOM uses are the `<video>`/`<audio>` elements and the simple `<input type="range">` element.
    - SQL injection
      - SQL injection is a code injection technique, used to attack data-driven applications.
      - SQL injections get their name from the SQL language and mainly target data stored in relational databases.
    - SQL
      - SQL stands for Structured Query Language and is a language used to create, update, retrieve and calculate data in table-based databases.
      - SQL databases use a relational database model and are particularly useful in handlind structured data with relations between different entities.
    - SSL
      - Secure Sockets Layer, commonly known as SSL or TLS, is a set of protocols and standards for transferring private data across the Internet.
      - SSL uses a cryptographic system that uses two keys to encrypt data.
    - Stream
      - A stream is a sequence of data made available over time, often due to network transmission or storage access times.
    - Strict mode
      - JavaScript's strict mode is a JavaScript feature that allows developers to use a more restrictive variant of JavaScript and it can be enabled by adding `'use strict';` at the very top of their code.
      - Strict mode elimiated some silent errors, might improve performance and changes the behavior of `eval` and `arguments` among other things.
    - String
      - Strings are one of the primitive data types in JavaScript.
      - They are sequences of characters and are used to represent text.
    - SVG
      - SVG stands for Scalable Vector Graphics and is a 2D vector image format based on an XML syntax.
      - SVG images can scale infinitely and can utilize clipping, masking, filters, animations etc.
    - Template literals
      - Template literals are strings that allow embedded expressions.
      - They support multi-line strings, expression interpolation and nesting.
    - TypeScript
      - TypeScript is a superset of JavaScript, adding optional static typing to the language.
      - TypeScript compiles to plain JavaScript.
    - Unit testing
      - Unit testing is a type of software testing, used to test individual units/components of a software.
      - The purpose of unit tests are to validate that each individual unit/component performs as designed.
    - URI
      - URI stands for Uniform Resource Identifier and is a text string referring to a resource.
      - A common type of URI is a URL, which is used for the identification of resources on the Web.
    - URL
      - URL stands for Uniform Resource Locator and is a text string specifying where a resource can be found on the Internet.
      - In the HTTP protocol, URLs are the same as web addresses and hyperlinks.
    - UTF-8
      - UTF-8 stands for UCS Transformation Format 8 and is a commonly used character encoding.
      - UTF-8 is backwards compatible with ASCII and can represent any standard Unicode character.
    - Value vs reference
      - When passing a variable by value, a copy of the variable is made, meaning that any changes made to the contents of the variable will not be reflected in the original variable.
      - When passing a variable by reference, the memory address of the actual variable is passed to the function or variable, meaning that modifying the variable's contents will be reflected in the original variable.
      - In JavaScript primitive data types are passed by value while objects are passed by reference.
    - Variable
      - A variable is a storage location, associated with an identifier and containing a value.
      - The value of a variable can be referred using the identifier and can be altered during execution.
    - Viewport
      - A viewport is a polygonal (usually rectangular) area in computer graphics that is currently being viewed.
      - In web development and design, it refers to the visible part of the document that is being viewed by the user in the browser window.
    - Vue
      - Vue.js is a progressive frontend framework for building user interfaces.
      - Vue.js separates view and state, utilizing a virtual DOM to update the user interface.
    - Software package metrics
      - Number of classes and interfaces
        - The number of concrete and abstract classes (and interfaces) in the package is an indicator of the extensibility of the package.
      - Afferent Couplings (Ca)
        - The number of classes in other packages that depend upon classes within the package is an indicator of the package's responsibility. Afferent = incoming.
        - A class afferent couplings is a measure of how many other classes use the specific class.
      - Efferent Couplings (Ce)
        - The number of classes in other packages that the classes in the package depend upon is an indicator of the package's dependence on externalities. Efferent = outgoing.
        - A class efferent couplings is a measure of how many different classes are used by the specific class.
      - Abstractness (A)
        - The ratio of the number of abstract classes (and interfaces) in the analyzed package to the total number of classes in the analyzed package.
        - The range for this metric is 0 to 1, with A=0 indicating a completely concrete package and A=1 indicating a completely abstract package.
      - Instability (I)
        - The ratio of efferent coupling (Ce) to total coupling (Ce + Ca) such that I = Ce / (Ce + Ca).
        - This metric is an indicator of the package's resilience to change.
        - The range for this metric is 0 to 1, with I=0 indicating a completely stable package and I=1 indicating a completely unstable package.
      - Distance from the main sequence (D)
        - The perpendicular distance of a package from the idealized line A + I = 1.
        - D is calculated as D = | A + I - 1 |
        - This metric is an indicator of the package's balance between abstractness and stability.
        - A package squarely on the main sequence is optimally balanced with respect to its abstractness and stability.
        - Ideal packages are either completely abstract and stable (I=0, A=1) or completely concrete and unstable (I=1, A=0).
        - The range for this metric is 0 to 1, with D=0 indicating a package that is coincident with the main sequence and D=1 indicating a package that is as far from the main sequence as possible.
      - Package dependency cycles
        - Package dependency cycles are reported along with the hierarchical paths of packages participating in package dependency cycles.
    - WebAssembly
      - WebAssembly (WA) is a web standard that defines an assembly-like text format and corresponding binary format for executalbe code in web pages.
      - WebAssembly is meant to complement JavaScript and improve its performance to match native code performance.
      - WebAssembly is a language for a conceptual machine that’s the least common denominator of the popular real world hardware.
      - Benefits
        - Speed
          - Its binaries are much smaller than textual JavaScript files. Because of their size, they are faster to download and this is especially important on slow networks.
        - Portability
          - To run an application on a device, it has to be compatible with the device’s processor architecture and operating system. That means compiling source code for every combination of operating system and CPU architecture that you want to support.
          - With WebAssembly there is only one compilation step and your app will run in every modern browser.
        - Flexibility
          - With WebAssembly, web developers will be able to choose other languages and more developers will be able to write code for the web.
          - When optimizing performance in existing JS apps, bottlenecks could be rewritten in a language that is better suited for the problem.
    - Web Components
      - Web Components are a set of web platform APIs that allow you to create new custom, reusable, encapsulated HTML tags to use on web pages and apps.
      - Building custom components using these standards means that you can use them across modern browsers regardless of any JavaScript library or framework.
    - WebGL
      - WebGL stands for Web Graphics Library and is a JavaScript API that can be used for drawing interactive 2D and 3D graphics.
      - WebGL is based on OpenGL and can be invoked within HTML `<canvas>` elements, which provide a rendering surface.
    - WebRTC
      - WebRTC stands for Web Real-Time Communication and is an API that can be used for video-chat, voice-calling and P2P-file-sharing web apps.
    - WebSockets
      - WebSockets is a protocol that allows for a persistent client-server TCP connection.
      - The WebSocket protocol uses lower overheads, facilitating real-time data transfer between client and server.
    - XHTML
      - XHTML stands for EXtensible HyperText Markup Language and is a language used to structure web pages.
      - XHTML is a reformulation of the HTML document structure as an application of XML.
    - XML
      - XML stands for eXtensible Markup Language and is a generic markup language specified by the W3C.
      - XML documents are plaintext documents structured with user-defined tags, surrounded by `<>` and optionally extended with attributes.
    - Yarn
      - Yarn is a package manager made by Facebook.
      - It can be used as an alternative to the npm package manager and is compatible with the public NPM registry.
  - Agile Manifesto
    - Values
      - Individuals and Interactions Over Processes and Tools
      - Working Software Over Comprehensive Documentation
      - Customer Collaboration Over Contract Negotiation
      - Responding to Change Over Following a Plan
    - Principles
      - Customer satisfaction through early and continuous software delivery
        - Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.
        - Customers are happier when they receive working software at regular intervals, rather than waiting extended periods of time between releases.
      - Accommodate changing requirements throughout the development process
        - Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.
        - The ability to avoid delays when a requirement or feature request changes.
      - Frequent delivery of working software
        - Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.
        - Scrum accommodates this principle since the team operates in software sprints or iterations that ensure regular delivery of working software.
      - Collaboration between the business stakeholders and developers throughout the project
        - Business people and developers must work together daily throughout the project.
        - Better decisions are made when the business and technical team are aligned.
      - Support, trust, and motivate the people involved
        - Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.
        - Motivated teams are more likely to deliver their best work than unhappy teams.
      - Enable face-to-face interactions
        - The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.
        - Communication is more successful when development teams are co-located.
      - Working software is the primary measure of progress
        - Working software is the primary measure of progress.
        - Delivering functional software to the customer is the ultimate factor that measures progress.
      - Agile processes to support a consistent development pace
        - Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.
        - Teams establish a repeatable and maintainable speed at which they can deliver working software, and they repeat it with each release.
      - Attention to technical detail and design enhances agility
        - Continuous attention to technical excellence and good design enhances agility.
        - The right skills and good design ensures the team can maintain the pace, constantly improve the product, and sustain change.
      - Simplicity
        - Simplicity--the art of maximizing the amount of work not done--is essential.
        - Develop just enough to get the job done for right now.
      - Self-organizing teams encourage great architectures, requirements, and designs
        - The best architectures, requirements, and designs emerge from self-organizing teams.
        - Skilled and motivated team members who have decision-making power, take ownership, communicate regularly with other team members, and share ideas that deliver quality products.
      - Regular reflections on how to become more effective
        - At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.
        - Self-improvement, process improvement, advancing skills, and techniques help team members work more efficiently.
    - Doing Agile vs. Being Agile
      - Doing Agile
        - Rules and Procedures
        - Knowledge intellectual
        - Skills
        - One Truth
      - Being Agile
        - Principles and Values
        - Knowledge intuitive
        - Beliefs and values
        - Multiple truths possible
    - Toffler's wave theory
      - The First Wave is the settled agricultural society which prevailed in much of the world after the Neolithic Revolution, which replaced hunter-gatherer cultures.
      - The Second Wave is Industrial Age society. The Second Wave began in Western Europe with the Industrial Revolution, and subsequently spread across the world. Key aspects of Second Wave society are the nuclear family, a factory-type education system and the corporation.
      - The Third Wave is the post-industrial society. Toffler says that since the late 1950s most countries have been transitioning from a Second Wave society into a Third Wave society. He coined many words to describe it and mentions names invented by others, such as the Information Age.
    - Spiral Dynamics
      - The Emergent, Cyclical, DoubleHelix Model Of The Adult Human Biopsychosocial Systems.
      - Levels
        - BEIGE Instinctive/Survivalistic
          - Do what you must just to stay alive
            - Uses instincts and habits just to survive
            - Distinct self is barely awakened or sustained
            - Food, water, warmth, sex, and safety have priority
            - Forms into survival bands to perpetuate life
            - Lives “off the land” much as other animals
        - PURPLE Magical/Animistic
          - Keep the spirits happy and the tribe’s nest warm and safe
            - Obeys the desires of the spirit being and mystical signs
            - Shows allegiance to chief, elders, ancestors, and the clan
            - Individual subsumed in group
            - Preserves sacred objects, places, events, and memories
            - Observes rites of passage, seasonal cycles, and tribal customs 
        - RED Impulsive/Egocentric
          - Be what you are and do what you want, regardless
            - The world is a jungle full of threats and predators
            - Breaks free form any domination or constraint to please self as self desires
            - Stands tall, expects attention, demands respect, and calls the shots
            - Enjoys self to the fullest right now without guilt or remorse
            - Conquers, out-foxes, and dominates other aggressive characters 
        - BLUE Purposeful/Authoritarian
          - Life has meaning, direction, and purpose with predetermined outcomes
            - One sacrifices self to the transcendent Cause, Truth, or righteous Pathway
            - The Order enforces a code of conduct based on eternal, absolute principles
            - Righteous living produces stability now and guarantees future reward
            - Impulsivity is controlled through guilt; everybody has their proper place
            - Laws, regulations, and discipline build character and moral fiber
        - ORANGE Achievist/Strategic
          - Act in your own self-interest by playing the game to win
            - Change and advancement are inherent within the scheme of things
            - Progresses by learning nature’s secrets and seeking out best solutions
            - Manipulates Earth’s resources to create and spread the abundant good life
            - Optimistic, risk-taking, and self-reliant people deserve success
            - Societies prosper through strategy, technology, and competitiveness 
        - GREEN Communitarian/Egalitarian
          - Seek peace within the inner self and explore, with others, the caring dimensions of community
            - The human spirit must be freed from greed, dogma, and divisiveness
            - Feelings, sensitivity, and caring supersede cold rationality
            - Spreads the Earth’s resources and opportunities equally among all
            - Reaches decisions through reconciliation and consensus processes
            - Refreshes spirituality, brings harmony, and enriches human development 
        - YELLOW Integrative
          - Live fully and responsibly as what you are and learn to become
            - Life is a kaleidoscope of natural hierarchies, systems, and forms
            - The magnificence of existence is valued over material possessions
            - Flexibility, spontaneity, and functionality have the highest priority
            - Differences can be integrated into interdependent, natural flows
            - Understands that chaos and change are natural
        - TURQUISE Holistic
          - Experience the wholeness of existence through mind and spirit
              - The world is a single, dynamic organism with its own collective mind
              - Self is both distinct and a blended part of a larger, compassionate whole
              - Everything connects to everything else in ecological alignments
              - Energy and information permeate the
            Earth’s total environment
              - Holistic, intuitive thinking and cooperative actions are to be expected 
    - Agile and Spiral Dynamics
      - Value System
        - Individual
        - Team
        - Organization
        - Environment
      - Levels
        - BLUE Purposeful/Authoritarian
          - Name: Procedural Agile
          - Right Use: Life critical applications
          - Strong: Order and Control
          - Weakness: Slow and Inflexible
          - Danger: unstoppable growth of procedures
          - Typical
            - One Truth, hierarchical, inflexible, silo thinking, one specific method, reacts slow to changes, guidelines, templates, procedures, disciplined
          - To keep it agile
            - Something in, something out
        - ORANGE Achievist/Strategic
          - Name: Money Driven Agile
          - Right Use: Direct financial crisis
          - Strong: Quick win and Improving
          - Weakness: Long term
          - Danger: Burning up people
          - Typical
            - More with less, success, improves, competition, outsmart others, situational ethics, drive
          - To keep it agile
            - Limit the maximum cost reduction / growth
        - GREEN Communitarian/Egalitarian
          - Name: People Centric Agile
          - Right Use: Political sensitive projects
          - Strong: Team thinking and involvement
          - Weakness: Paralysis through consensus
          - Danger: To many people involved
          - Typical
            - People first, consensus, everybody is equal, flat organization, team effort, acceptance, slow in taking decisions
          - To keep it agile
            - Limit number of stakeholders
        - YELLOW Integrative
          - Name: Effective Agile
          - Right Use: Chaotic and global environments
          - Strong: Respond to change
          - Weakness: Seen as unstable
          - Danger: Loosing the rest
          - Typical
            - Effectiveness over efficiency, people are different, embrace change, improve, out-of-the-box, wolf in sheep’s clothing, hard to understand, can loose interest, no interest in status, long term thinking
      - Value Systems and Agile Manifesto 
        - BLUE Value System 
          - Comprehensive documentation over working software
          - Following a plan over responding to change
          - Processes and tools over individuals and interactions
        - ORANGE Value System
          - Contract negotiation over customer collaboration
          - Working software over comprehensive documentation
        - GREEN Value System
          - Individuals and interactions over processes and tools
          - Customer collaboration over contract negotiation
        - YELLOW Value System 
          - Responding to change over following a plan
      - Transform from Doing to Being
        - Making the transition from one color to the next From BLUE to ORANGE Value System 
          - Look for what to keep
          - Make use of the dominant color to implement the new one
          - Don’t skip colors (there is no shortcut)
          - Acknowledge the past and show that the environment has changed
          - Emphasize signs of the new color when they occur
          - Make sure the environment supports the new color
          - Use Real Life 
        - From BLUE to ORANGE Value System 
          - From doing your duty to making a success
            - Use the higher authority
            - Design new leaner procedures
            - Design new procedures which embed continuous improvement
            - Reward results instead of doing your tasks
            - Flatten organizational layers
            - Add competition
        - From ORANGE to GREEN Value System 
          - From personal success to shared goals
            - Use scientific research to persuade new way of working
            - Make rewards team-oriented
            - Make competition team-oriented
            - Reward people for growing and caring not only for getting successes
            - Remove the boss/manager and let the group make important decisions 
        - From GREEN to YELLOW Value System 
          - From all equal to all unique value
            - Make rewards for contribution to team growth and wellbeing
            - Remove consensus thinking and introduce situational leadership
            - Make an ecological mission and/or vision statement to inspire
            - Remove strict Function/Job Descriptions 
  - United Nations Sustainable Development Goals
    - SDG 1: No Poverty
      - End poverty in all its forms everywhere
    - SDG 2: No Hunger
      - End hunger, achieve food security and improved nutrition, and promote sustainable agriculture
    - SDG 3: Good Health and Well-being
      - Ensure healthy lives and promote well-being for all at all ages
    - SDG 4: Quality Education
      - Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all
    - SDG 5: Gender Equality
      - Achieve gender equality and empower all women and girls
    - SDG 6: Clean Water and Sanitation
    - Ensure availability and sustainable management of water and sanitation for all
    - SDG 7: Affordable and Clean Energy
      - Assure access to affordable, reliable, sustainable and modern energy for all
    - SDG 8: Decent Work and Economic Growth
      - Promote sustained, inclusive and sustainable economic growth, full and productive employment, and decent work for all
    - SDG 9: Industry, Innovation, and Infrastructure
      - Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation
    - SDG 10: Reduced Inequalities
      - Reduce inequality within and among countries
    - SDG 11: Sustainable Cities and Communities
      - Make cities and human settlements inclusive, safe, resilient, and sustainable
    - SDG 12: Responsible Consumption and Production
      - Ensure sustainable consumption and production patterns
    - SDG 13: Climate Action
      - Take urgent action to combat climate change and its impacts
    - SDG 14: Life Below Water
      - Conserve and sustainably use the oceans, seas, and marine resources for sustainable development
    - SDG 15: Life on Land
      - Protect, restore, and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss
    - SDG 16: Peace, Justice, and Strong Institutions
      - Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable, and inclusive institutions at all levels
    - SDG 17: Partnership for the Goals
      - Strengthen the means of implementation and revitalize the global partnership for sustainable development
  - Reactive Manifesto
  - GitOps
    - Git as the Source of Truth 
      - Provisioning of resources and deployment of k8s is declarative
      - Entire system state is under version control and described in a single Git repository   
      - Operational changes are made by pull request (plus build & release pipelines)
      - Diff tools detect any divergence and notify us via Slack alerts; and sync tools enable convergence
      - Rollback and audit logs are also provided via Git  
  -  Cloud Native Ecosystem
    - Value Proposition
      - Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds.
      - Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.
      - These techniques enable loosely coupled systems that are resilient, manageable, and observable.
      - Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.
      - The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendorneutral projects.
    - Patterns
    
    1. CONTAINERIZATION
          - Commonly done with Docker containers
          - Any size application and dependencies (even PDP-11
 code running on an emulator) can be containerized
          - Over time, you should aspire towards splitting suitable
 applications and writing future functionality as microservices
2. CI/CD
          - Setup Continuous Integration/Continuous Delivery
 (CI/CD) so that changes to your source code
 automatically result in a new container being
 built, tested, and deployed to staging and
 eventually, perhaps, to production
          - Setup automated rollouts, roll backs and testing
3. ORCHESTRATION &
APPLICATION DEFINITION
          - Kubernetes is the market-leading orchestration solution
          - You should select a Certified Kubernetes Distribution,
 Hosted Platform, or Installer: cncf.io/ck
          - Helm Charts help you define, install, and upgrade
 even the most complex Kubernetes application
4. OBSERVABILITY & ANALYSIS
          - Pick solutions for monitoring, logging and tracing
          - Consider CNCF projects Prometheus for monitoring,
 Fluentd for logging and Jaeger for Tracing
          - For tracing, look for an OpenTracing-compatible
 implementation like Jaeger
5. SERVICE PROXY, DISCOVERY, & MESH
          - CoreDNS is a fast and flexible tool that
 is useful for service discovery
          - Envoy and Linkerd each enable service
 mesh architectures
          - They offer health checking, routing,
 and load balancing
7. DISTRIBUTED DATABASE & STORAGE
When you need more resiliency and scalability than
you can get from a single database, Vitess is a good
option for running MySQL at scale through sharding.
Rook is a storage orchestrator that integrates a
diverse set of storage solutions into Kubernetes.
Serving as the "brain" of Kubernetes, etcd provides a
reliable way to store data across a cluster of machines.
9. CONTAINER REGISTRY & RUNTIME
Harbor is a registry that stores, signs, and scans content.
You can use alternative container runtimes. The most common,
all of which are OCI-compliant, are containerd, rkt and CRI-O. 
8. STREAMING & MESSAGING
When you need higher performance than JSON-REST, consider
using gRPC or NATS. gRPC is a universal RPC framework. NATS is
a multi-modal messaging system that includes request/reply,
pub/sub and load balanced queues.
6. NETWORKING
To enable more flexible networking,
use a CNI-compliant network project
like Calico, Flannel, or Weave Net.
10. SOFTWARE DISTRIBUTION
If you need to do secure software distribution,
evaluate Notary, an implementation of The
Update Framework.

  - Blockchain Landscape
    - Bitcoin
      - The Bitcoin protocol allows users to exchange of the Bitcoin (BTC) cryptocurrency without any trusted third party using the decentralized, peer-to-peer Bitcoin network.
      - The record of the transfer is maintained in a public blockchain, also known as the Bitcoin blockchain.
      - The blockchain is replicated on every node that participates in the Bitcoin network.
    - Bitcoin Blockchain Features
      - The Bitcoin Blockchain is the shared ledger, which can be read and modified by all involved participants in the blockchain.
      - Public ledger
        - Any node with a Bitcoin client can read or add transactions to the ledger.
      - Permissionless entry
        - The miner who has solved the Proof-of-Work is the node that will finalize a block representing a set of transactions in the ledger.
        - Any node can be a miner by solving the PoW puzzle successfully.
      - Proof-of-Work
        - Bitcoin uses the computation of the hash with a certain difficulty level as a Proof-of-Work measure.
      - Customizable fields
        - The structure of the blockchain is fixed. Each block has a number of transactions with some meta-data where the user can add some information.
      - Native token
        - The native token of the Bitcoin blockchain is the Bitcoin, which is also the cryptocurrency that is moved between users.
        - Bitcoins and transaction fees in Bitcoins are awarded to the miner mining a particular block.
    - The Blockchain
      - While Bitcoin used the blockchain to implement a cryptocurrency system, it can be generically applied in other scenarios, with or without using a native token.
      - The blockchain offers several features that make it an enabler for new applications that were either tedious or impossible to implement in its absence.
    - Blockchain features
      - Cryptographic guarantees
        - All transactions on the blockchain are signed by end-users. This cryptographic guarantee allows for verification of the transaction with the user’s public key.
        - Signing guarantees the authenticity, integrity and non-repudiation of that transaction.
      - Pseudonymity
        - Each entity within the blockchain network transacts with a generated address, which does not reveal the real identity of the user.
        - This allows a certain amount of privacy on all transactions.
      - Immutability
        - The transaction broadcasted to the blockchain network has to be confirmed and included as part of the distributed ledger.
        - Once confirmed, the transaction cannot be changed and stays in the ledger forever.
        - No entity can delete or rollback transactions once they are included as part of the distributed ledger.
      - Shared Read and Write
        - The blockchain is either public or private within a limited set of entities.
        - All concerned participants have visibility into the blockchain and can independently verify any transaction within it.
        - All participants can generate transactions that can be added to the shared ledger.
      - Auditability and Transparency
        - All transactions on the blockchain are validated and timestamped after the transaction is verified and included in the distributed ledger with distributed consensus.
        - This accounts for a global truth that any node in the future can verify and no node in the network can change the data that is part of this distributed ledger.
        - This improves accountability as well as transparency for the data that is included in the blockchain.
      - Distributed ownership
        - In case of public blockchains, no entity owns the blockchain but all of them can add to the ledger and validate transactions.
        - In a consortium blockchain, all participants own the blockchain equally and they can change the ledger by a super-majority of votes or other forms of distributed consensus algorithms.
    - Blockchain Platforms
      - Bitcoin based meta-data platforms
        - Designed to leverage the already adopted Bitcoin blockchain.
        - These platforms allow for allotment and transfer of custom assets using the Bitcoin blockchain.
      - Blockchain platforms for financial applications
        - Also known as FinTech blockchain platforms, this category specifically targets applications within the financial domain.
      - Smart contract platforms 
        - These platforms mainly focus on applications that require complex logic beyond just expressing account balances and balance transfers as in the case of cryptocurrency transfers.
      - Consortium/Enterprise platforms
        - Target enterprises and consortiums that wish to take advantages of the blockchain, but in a more controlled manner. 
        - These also typically use a distributed consensus protocol, getting completely rid of PoW and mining.
      - Sidechain platforms
        - Sidechain platforms allow for faster innovation without polluting the main Bitcoin blockchain or incurring its overhead.
        - Sidechains allow for building alternate chains that operate via a two-way peg into the Bitcoin blockchain or as an anchored chain.
    - Use Cases
      - Transactions & Payment Services
        - Wallets
        - Merchant Transactions
        - Smart Contracts
        - International Payments & Remitances
        - Cryptocurrencies
      - Exchanges & Trading
        - Prediction Markets
        - Clearing & Settlement
        - Marketplaces
        - P2P Lending
        - Crowdfunding Platforms & Tokenization
        - Crypto-Exchange
        - Crypto-Investment
      - Identity, Authentication & Security
        - Data & Document Autentification
        - Security
        - Digital Identity
      - Enterprise blockchain solutions
        - Financial Services
        - Supply Chain
        - Healthcare & Insurance
        - Compliance
      - Social, networks & games
        - Games
        - Social & Networks
      - Ecosystem
        - Mining
        - Hardware & Data Storage
        - Infrastructure & Application Development
  - Cloud Native Landscape 
    - App Definition and Development
      - Database
      - Streaming & Messaging
      - Application Definition & Image Build
      - Continuous Integration & Delivery
    - Orchestration & Management
      - Scheduling & Orchestration
      - Coordination & Service Discovery
      - Remote Procedure Call
      - Service Proxy
      - API Gateway
      - Service Mesh
    - Runtime
      - Cloud-Native Storage
      - Container Runtime
      - Cloud-Native Network
    - Provisioning
      - Automation & Configuration
      - Container Registry
      - Security & Compliance
      - Key Management
    - Cloud
      - Public
    - Platform
      - Certified Kubernetes - Distribution
      - Certified Kubernetes - Hosted
      - Certified Kubernetes - Installer
      - PaaS/Container Service
    - Observability and Analysis
      - Monitoring
      - Logging
      - Tracing
      - Chaos Engineering
    - Special
      - Kubernetes Certified Service Provider
      - Kubernetes Training Partner
  - Serverless Landscape
    - Framework
    - Security
    - Platform
      - Hosted
      - Installable
  - Deployment Strategies
    - Recreate
      - Terminate the old version and release the new one
    - Ramped
      - Release a new version on a rolling update fashion, one after the other
    - Blue/Green
      - Release a new version alongside the old version then switch traffic
    - Canary
      - Release a new version to a subset of users, then proceed to a full rollout
    - A/B testing
      - Release a new version to a subset of users in a precise way (HTTP headers, cookie, weight, etc.).
      - This doesn’t come out of the box with Kubernetes, it imply extra work to setup a smarter loadbalancing system (Istio, Linkerd, Traeffik, custom nginx/haproxy, etc).
    - Shadow
      - Release a new version alongside the old version. Incoming traffic is mirrored to the new version and doesn't impact the response.
  - Reactive programming
    - Programming with asynchronous data streams
  - Multicloud Strategy
    - Avoid vendor lock-in
      - Choose a portable software stack.
        - "Organizations need to pick services and technology stacks that are available across different vendors and platforms," Andreas Grabner, DevOps activist at Dynatrace, told Datamation. "One of the reasons why Kubernetes, OpenShift and CloudFoundry are so popular is that they can run on all cloud stacks."
      - Use containers.
        - Along the same lines, deploying containers also makes workloads more portable. Because container technology packages an application together with all of its dependencies, it makes moving applications from one environment to another much easier.
      - Learn about vendors' services before committing.
        - Before putting a workload in a particular cloud, consider whether you'll be able to move that workload. Serverless services like Amazon Lambda and Microsoft Azure Functions offer benefits like faster development and deployment, but they also dramatically increase the possibility of vendor lock-in. Enterprises need to carefully weigh the potential benefits and risks.
      - Avoid a siloed approach. Simply housing one workload on one cloud platform and another on a different platform is not utilizing a multicloud strategy to its full potential. Rather, businesses must ensure that these efforts are connected. This enables them to benefit from the flexibility and cost savings of moving workloads around as needed to meet unique business requirements.
    - Optimize costs
      - Match workloads to vendors.
        - While public cloud vendors often offer similar prices for similar services, that isn’t always the case. Customers may be able to find better prices — or better services — by shopping around. It's always best to approach each new cloud project on a case-by-case basis.
      - Adopt microservices architecture.
        - For new, "cloud-first" applications, many organizations find that microservices architecture, which divides applications into very small pieces that can be reused for different projects, speeds development, decreasing overall costs. Often microservices applications also rely on serverless computing services, which can further reduce time and expenses. However, as already mentioned, the potential cost benefit of these services should be weighed against the risk of vendor lock-in.
      - Consider a cloud services broker.
        - For a large enterprise, the job of finding the right cloud vendor and service for each cloud workload could require a dedicated team. Cloud services brokers provide this service via software and/or consultants, and in some cases they may be less expensive than selecting the cloud service provider yourself. In addition, some brokers also offer integration or management services that can further reduce your total cost of ownership (TCO).
      - Investigate cost optimization tools. 
        - imilar to cloud services brokerages, cost optimization tools find the best deal for each cloud workload. Many of these software tools can also perform functions like shutting down unused instances, rightsizing instances or moving to different payment options that can reduce costs further.
      - Negotiate with vendors.
        - Public cloud vendors offer upfront pricing, but that doesn't mean you have to pay the list price. Large enterprises with an abundance of workloads to move to the cloud may be able to get a better deal by contacting the cloud providers directly.
    - Design for reliability
      - Do your due diligence.
        - Before you sign up for a cloud service, make sure you understand your service level agreement, including any promised levels of uptime and how the vendor will compensate you if it fails to live up to that agreement. Also, be aware that some providers charge different prices for different levels of availability. Make sure you choose the tier that best meets the needs of each particular workload.
      - Plan for disaster recovery and business continuity (DR/BC).
        - A multicloud strategy can be an integral part of your DR/BC plans because it gives you the option of moving workloads from one data center and vendor to another in the case of an emergency. However, organizations need to make sure that their DR/BC plans cover all their cloud vendors and that they have tested their plans before a disaster happens.
      - Don't forget about backup and archive.
        - Your cloud-based data and applications need to be backed up, too. Many security experts say it's a good idea for organizations to use a different cloud vendor for backup and primary systems.
      - Consider geography when choosing vendors and data centers.
        - A multicloud approach won't help with availability if all your vendors are running your workloads in Virginia data centers when a hurricane strikes Virginia. Organizations need to find a balance: Nearby data centers can provide lower latency and better performance, but geographically dispersed data centers offer lower risk in the case of a disaster.
    - Ensure security and compliance
      - Review relevant regulations.
        - This year's adoption of the European Union's General Data Protection Regulation (GDPR) has encouraged many enterprises to revisit their compliance. Your organization needs to be aware of any laws or industry mandates that govern your data and ensure that any vendors you use can meet your regulatory needs.
      - Control access to data and applications.
        - When using multiple cloud vendors, ensuring that only the right people can access sensitive data becomes more challenging. Many enterprises choose to use an identity and access management (IAM) solution to make this problem more manageable.
      - Encrypt data.
        - No matter how secure your systems are or what kind of defensive measures your various cloud vendors have in place, you need to make sure that your data is encrypted both at rest and in transit. This is particularly important in a multicloud situation where workloads are moving from one cloud to another, because users sometimes forget about the need to encrypt data in motion.
      - Deploy appropriate security measures.
        - Your cybersecurity team needs to make sure that the other protections you have on your networks are adequate to protect a multicloud environment. Defending the perimeter is no longer enough when you are in a multicloud situation; you need to make sure that you have an agile defense and can detect and mitigate attacks in progress.
      - Use a private cloud as necessary.
        - For some data and applications, no public cloud will be adequate to meet your security and compliance needs. As a result, some organizations will always need to run a private cloud.
    - Manage multiple providers
      - Rely on automation and orchestration.
        - Many of the automation tools used by DevOps teams also make life easier when dealing with multicloud architecture.
      - Use containers.
        - Having applications packaged with all their dependencies increases portability and can simplify management.
      - Consider a cloud service broker.
        - These tools and services can help you match workloads to the most appropriate cloud vendor.
      - Invest in a multicloud management tool.
        - Having a single pane of glass to manage all of your clouds can be invaluable in keeping costs under control, and tools that make use of artificial intelligence can be particularly helpful.
      - Create an API management strategy.
        - Multicloud architecture means dealing with many different APIs from different vendors; you'll need a plan for dealing with them.
      - Keep integration in mind.
        - When selecting vendors or creating new applications, you'll need to consider how you'll integrate data and applications residing in different clouds.
      - Build your in-house team.
        - You'll need IT pros with strong cloud computing skills to manage your multicloud architecture, so plan to build out your team over time.
      - Reevaluate regularly.
        - Just because a particular vendor was the best choice for a particular workload at one point in time doesn't mean that this will continue to be the case forever. Public cloud vendors are adding capabilities and changing prices all the time, so you'll need to revisit your decisions on a regular basis.
    - Avoid complexity
      - Start with an inventory.
        - Before you can devise a plan to make multicloud environment more manageable, you need to know what workloads you have running and where.
        - A discovery tool can help you create an inventory and possibly highlight applications and data that are good candidates for moving to the cloud.
      - Deploy automation and orchestration tools.
        - The same automation and orchestration tools that DevOps teams are implementing in order to speed application delivery and improve uptime can also help tame multicloud complexity.
        - While these tools carry some expense, the time they can save generally makes them very worthwhile from a cost perspective.
      - Consider a multicloud management tool.
        - Depending on how complicated multicloud environment is, you might also want to consider a multicloud management tool.
        - These products are designed specifically for multicloud, and thus often do a better job than traditional IT management tools.
      - Control growth.
        - Whether you use a multicloud management tool or not, you need to make sure that you have policies, procedures and (preferably) software in place to prevent employees from spinning up new cloud instances as they see fit.
        - This kind of shadow IT can dramatically increase cloud costs while exposing the company to unnecessary risk.
    - Consider integration needs
      - API management strategy.
        - Using multiple cloud vendors means relying on a host of different APIs.
        - Make sure you have a plan for keeping track of those tools and managing their use.
      - Carefully select data and application integration tools.
        - In a multicloud environment you will almost certainly need some middleware to get applications to work together and get data where it needs to be in the proper format.
        - There are a host of different data integration tools on the market, so you'll need to do your homework to find the best fit for your needs.
    - Build in-house team
      - Hire experienced people.
        - When you can, you should hire people who have previously worked in multicloud environments.
      - Train existing staff.
  - Round‑robin load balancing
    - is one of the simplest methods for distributing client requests across a group of servers.
    - Going down the list of servers in the group, the round‑robin load balancer forwards a client request to each server in turn.
    - When it reaches the end of the list, the load balancer loops back and goes down the list again (sends the next request to the first listed server, the one after that to the second server, and so on).
    - The main benefit of round‑robin load balancing is that it is extremely simple to implement.
    - However, it does not always result in the most accurate or efficient distribution of traffic, because many round‑robin load balancers assume that all servers are the same: currently up, currently handling the same load, and with the same storage and computing capacity.
    - The following variants to the round‑robin algorithm take additional factors into account and can result in better load balancing:
    - Weighted round robin
      - A weight is assigned to each server based on criteria chosen by the site administrator; the most commonly used criterion is the server’s traffic‑handling capacity.
      - The higher the weight, the larger the proportion of client requests the server receives. If, for example, server A is assigned a weight of 3 and server B a weight of 1, the load balancer forwards 3 requests to server A for each 1 it sends to server B.
    - Dynamic round robin
      - A weight is assigned to each server dynamically, based on real‑time data about the server’s current load and idle capacity.
  - Cloud Security Software and Services
    - The following are some of the cloud security software and service options businesses should consider:
    - IaaS or PaaS cloud security options – these are add-on services that provide enterprises with more extensive security options than are available with basic cloud options.
    - Identity and Access Management (IAM) – these tools ensure that only authorized parties have access to data and computing resources.
    - Physical security – IaaS/PaaS providers should have physical security  – locked door, checkpoints – in addition to digital security to ensure their IT assets remain secure.
    - Encryption – encrypts data at rest and in motion.
    - Penetration testing – outside consultants are hired as “white hats” to break into a company’s system with the goal of identifying weaknesses.
    - Compliance controls – ensure adherence to HIPPA, GDPR, etc.